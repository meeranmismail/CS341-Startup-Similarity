{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### This notebook achieves the following objectives:\n",
    "\n",
    "    A) IDF-Vector Compute the IDF values for each word present in the corpus \n",
    "    of training samples (which in this case is the set of 3000 labeled companies)\n",
    "\n",
    "    B) Generate the graph. Compute the Jaccard similarity between \n",
    "    companies sharing similar high IDF words, and then populate the \n",
    "    graph where a node is a company and an edge is weighted by the \n",
    "    Jaccard similarity between the two companies (if calculated based on cutoff)\n",
    "\n",
    "    C) Run a simple test of taking two companies, placing all of \n",
    "    their weighted edges in a respective vector, then computing \n",
    "    the dot product between these two edges\n",
    "\n",
    "    Conclusion: We have found that the dot product method is \n",
    "    producing results in which the companies with the highest \n",
    "    dot products do in fact appear very similar. See part C for exact results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A) IDF Vector\n",
    "### To use an IDF vector, you have two options:\n",
    "\n",
    " #### Option 1: Use this code to generate a new IDF vector from words in the training dataset. This option has multiple cells to run\n",
    " \n",
    " #### Option 2: Use this code to read in a previously stored IDF vector. This option has one cell to run\n",
    "The currently implemented option 2 use case reads in the entire 650,000 companies from the raw data file and filters down to 100,000 companies which all have full pitch book descriptions\n",
    "             \n",
    " Note: Run either option 1 or option 2. If both are run, option 2 IDF vector will be used ebcause it will overwrite the variable set by option 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "class TfIdf:\n",
    "\n",
    "    \"\"\"Tf-idf class implementing http://en.wikipedia.org/wiki/Tf-idf.\n",
    "  \n",
    "     The library constructs an IDF corpus and stopword list either from\n",
    "     documents specified by the client, or by reading from input files.  It\n",
    "     computes IDF for a specified term based on the corpus, or generates\n",
    "     keywords ordered by tf-idf for a specified document.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_filename = None, stopword_filename = None,\n",
    "               DEFAULT_IDF = 1.5):\n",
    "        \"\"\"Initialize the idf dictionary.  \n",
    "    \n",
    "        If a corpus file is supplied, reads the idf dictionary from it, in the\n",
    "        format of:\n",
    "        # of total documents\n",
    "        term: # of documents containing the term\n",
    "\n",
    "        If a stopword file is specified, reads the stopword list from it, in\n",
    "        the format of one stopword per line.\n",
    "\n",
    "        The DEFAULT_IDF value is returned when a query term is not found in the\n",
    "        idf corpus.\n",
    "        \"\"\"\n",
    "        self.num_docs = 0\n",
    "        self.term_num_docs = {}     # term : num_docs_containing_term\n",
    "        self.stopwords = []\n",
    "        self.idf_default = DEFAULT_IDF\n",
    "\n",
    "        if corpus_filename:\n",
    "            corpus_file = open(corpus_filename, \"r\")\n",
    "\n",
    "          # Load number of documents.\n",
    "            line = corpus_file.readline()\n",
    "            self.num_docs = int(line.strip())\n",
    "\n",
    "          # Reads \"term:frequency\" from each subsequent line in the file.\n",
    "            for line in corpus_file:\n",
    "                tokens = line.split(\":\")\n",
    "                term = tokens[0].strip()\n",
    "                frequency = int(tokens[1].strip())\n",
    "                self.term_num_docs[term] = frequency\n",
    "\n",
    "        if stopword_filename:\n",
    "            stopword_file = open(stopword_filename, \"r\")\n",
    "            self.stopwords = [line.strip() for line in stopword_file]\n",
    "\n",
    "    def get_tokens(self, doc):\n",
    "        \"\"\"Break a string into tokens, preserving URL tags as an entire token.\n",
    "\n",
    "        This implementation does not preserve case.  \n",
    "        Clients may wish to override this behavior with their own tokenization.\n",
    "        \"\"\"\n",
    "        # Attempt 1 - Faster results (this one is faster than uncommented solution, but doesn't get rid of stop words)\n",
    "        #str_list = [word.lower().translate(str.maketrans(' ', ' ', string.punctuation)) for word in re.split('\\s|\\.|-|,',str(doc))]\n",
    "        \n",
    "        punctuation = '[^\\w\\s]'\n",
    "        doc = pd.Series(doc)\n",
    "        txt = doc.str.lower().str.replace(punctuation, ' ').str.cat(sep=' ')\n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        words = nltk.tokenize.word_tokenize(txt)\n",
    "        return set(words) - stopwords\n",
    "\n",
    "        \n",
    "    def add_input_document(self, input):\n",
    "        \"\"\"Add terms in the specified document to the idf dictionary.\"\"\"\n",
    "        self.num_docs += 1\n",
    "        words = set(self.get_tokens(input))\n",
    "        for word in words:\n",
    "            if word in self.term_num_docs:\n",
    "                self.term_num_docs[word] += 1\n",
    "            else:\n",
    "                self.term_num_docs[word] = 1\n",
    "\n",
    "    def save_corpus_to_file(self, idf_filename, stopword_filename,\n",
    "                          STOPWORD_PERCENTAGE_THRESHOLD = 0.01):\n",
    "        \"\"\"Save the idf dictionary and stopword list to the specified file.\"\"\"\n",
    "        output_file = open(idf_filename, \"w\")\n",
    "\n",
    "        output_file.write(str(self.num_docs) + \"\\n\")\n",
    "        for term, num_docs in self.term_num_docs.items():\n",
    "            output_file.write(term + \": \" + str(num_docs) + \"\\n\")\n",
    "\n",
    "        sorted_terms = sorted(self.term_num_docs.items(), key=itemgetter(1),\n",
    "                          reverse=True)\n",
    "        stopword_file = open(stopword_filename, \"w\")\n",
    "        for term, num_docs in sorted_terms:\n",
    "            if num_docs < STOPWORD_PERCENTAGE_THRESHOLD * self.num_docs:\n",
    "                break\n",
    "\n",
    "            stopword_file.write(term + \"\\n\")\n",
    "\n",
    "    def get_num_docs(self):\n",
    "        \"\"\"Return the total number of documents in the IDF corpus.\"\"\"\n",
    "        return self.num_docs\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        \"\"\"Retrieve the IDF for the specified term. \n",
    "    \n",
    "        This is computed by taking the logarithm of ( \n",
    "        (number of documents in corpus) divided by (number of documents\n",
    "        containing this term) ).\n",
    "        \"\"\"\n",
    "        if term in self.stopwords:\n",
    "            return 0\n",
    "\n",
    "        if not term in self.term_num_docs:\n",
    "            return self.idf_default\n",
    "\n",
    "        return math.log(float(1 + self.get_num_docs()) / (1 + self.term_num_docs[term]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Now read in the full raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#################### Full Data #########################\n",
    "training_categories_df = pd.read_csv(\"../../data/raw_data_fixed.csv\",  encoding = \"ISO-8859-1\", usecols=['domain'\\\n",
    ", 'tx_industry', 'cb_category', 'tx_category', 'cb_desc', 'pb_desc', 'pb_category'])\n",
    "\n",
    "#mydoclist = training_categories_df.ix[0:,'pb_desc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Filter out companies that do not have a pitch book provided description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 166406 companies that have a full pitch book provided description\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter out all companies that don't have a pitch book provided description\n",
    "subset_df_pb_desc = training_categories_df.ix[training_categories_df['pb_desc'].notnull()]\n",
    "\n",
    "\n",
    "# Now print out some info\n",
    "len_of_df = subset_df_pb_desc.shape[0]\n",
    "print(\"There are {} companies that have a full pitch book provided description\".format(len_of_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below line if you need to write the filtered companies to a csv\n",
    "#subset_df_pb_desc.to_csv(\"../../data/100000_companies_with_description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of final dataframe to use for building the graph: 100000\n"
     ]
    }
   ],
   "source": [
    "# Cut this down to 100,000 companies using the head function\n",
    "subset_df_pb_desc = subset_df_pb_desc.head(100000)\n",
    "print(\"Size of final dataframe to use for building the graph: {}\".format(subset_df_pb_desc.shape[0]))\n",
    "#subset_df_pb_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "mydoclist = subset_df_pb_desc['pb_desc'].values\n",
    "print(len(mydoclist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### This code computes the actual IDF vector and can take a couple minutes to run with 100,000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idfcalc = TfIdf()\n",
    "for entry in mydoclist:\n",
    "    idfcalc.add_input_document(entry)\n",
    "#print(idfcalc.term_num_docs)\n",
    "\n",
    "idf_vec = []\n",
    "term_vec = []\n",
    "for term in idfcalc.term_num_docs:\n",
    "    idf = idfcalc.get_idf(term)\n",
    "    idf_vec.append(idf)\n",
    "    term_vec.append(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Convert the IDF vector a pandas Series and sort by IDF value (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dioxin               10.819788\n",
      "spiegelmers          10.819788\n",
      "nanopositioning      10.819788\n",
      "schlumberger         10.819788\n",
      "handelsblad          10.819788\n",
      "nrc                  10.819788\n",
      "gassification        10.819788\n",
      "pyro                 10.819788\n",
      "tme                  10.819788\n",
      "ntag                 10.819788\n",
      "latticed             10.819788\n",
      "ntp                  10.819788\n",
      "neurotherapeutics    10.819788\n",
      "trickster            10.819788\n",
      "pangya               10.819788\n",
      "omnigen              10.819788\n",
      "nutricosmetics       10.819788\n",
      "acreages             10.819788\n",
      "protide              10.819788\n",
      "characteristic       10.819788\n",
      "epicheck             10.819788\n",
      "careful              10.819788\n",
      "chemetics            10.819788\n",
      "photosensitive       10.819788\n",
      "suscription          10.819788\n",
      "nanopositioners      10.819788\n",
      "cauterization        10.819788\n",
      "miltiple             10.819788\n",
      "colli                10.819788\n",
      "quadrantanopia       10.819788\n",
      "                       ...    \n",
      "allows                2.749195\n",
      "information           2.744828\n",
      "helps                 2.741410\n",
      "systems               2.741100\n",
      "web                   2.703819\n",
      "service               2.700687\n",
      "applications          2.694601\n",
      "business              2.694157\n",
      "manufacturer          2.558908\n",
      "enables               2.548496\n",
      "data                  2.471961\n",
      "also                  2.324841\n",
      "application           2.321370\n",
      "operator              2.307407\n",
      "technology            2.242912\n",
      "mobile                2.222214\n",
      "users                 2.193920\n",
      "develops              2.162138\n",
      "management            2.041616\n",
      "based                 2.014788\n",
      "software              1.951868\n",
      "products              1.943872\n",
      "online                1.682341\n",
      "developer             1.370470\n",
      "provides              1.294455\n",
      "platform              1.288343\n",
      "offers                1.277881\n",
      "services              1.135172\n",
      "provider              0.647687\n",
      "company               0.026754\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "idf_vector = pd.Series(idf_vec, index=term_vec)\n",
    "\n",
    "sorted_idf_vector = idf_vector.sort_values(ascending=False)\n",
    "idf_vector = sorted_idf_vector\n",
    "print(idf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Uncomment the below line to save the IDF vector to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This code saves the IDF vector to a file\n",
    "# idf_vector.to_csv(\"../../data/100000_companies_with_description_idf_vector.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### End Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Option 2 (skip this if option 1 was utilized, else proceed to read an IDF vector from a csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dioxin</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spiegelmers</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nanopositioning</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schlumberger</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handelsblad</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nrc</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gassification</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyro</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tme</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntag</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latticed</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntp</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neurotherapeutics</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trickster</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pangya</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>omnigen</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nutricosmetics</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acreages</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protide</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>characteristic</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epicheck</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>careful</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chemetics</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photosensitive</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suscription</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nanopositioners</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cauterization</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miltiple</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colli</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quadrantanopia</th>\n",
       "      <td>10.819788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allows</th>\n",
       "      <td>2.749195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information</th>\n",
       "      <td>2.744828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helps</th>\n",
       "      <td>2.741410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systems</th>\n",
       "      <td>2.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web</th>\n",
       "      <td>2.703819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>2.700687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applications</th>\n",
       "      <td>2.694601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>2.694157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer</th>\n",
       "      <td>2.558908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enables</th>\n",
       "      <td>2.548496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>2.471961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>2.324841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application</th>\n",
       "      <td>2.321370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator</th>\n",
       "      <td>2.307407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>2.242912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile</th>\n",
       "      <td>2.222214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>users</th>\n",
       "      <td>2.193920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>develops</th>\n",
       "      <td>2.162138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>2.041616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>based</th>\n",
       "      <td>2.014788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>1.951868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>products</th>\n",
       "      <td>1.943872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online</th>\n",
       "      <td>1.682341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developer</th>\n",
       "      <td>1.370470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provides</th>\n",
       "      <td>1.294455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platform</th>\n",
       "      <td>1.288343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offers</th>\n",
       "      <td>1.277881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>services</th>\n",
       "      <td>1.135172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provider</th>\n",
       "      <td>0.647687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.026754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49813 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         IDF\n",
       "dioxin             10.819788\n",
       "spiegelmers        10.819788\n",
       "nanopositioning    10.819788\n",
       "schlumberger       10.819788\n",
       "handelsblad        10.819788\n",
       "nrc                10.819788\n",
       "gassification      10.819788\n",
       "pyro               10.819788\n",
       "tme                10.819788\n",
       "ntag               10.819788\n",
       "latticed           10.819788\n",
       "ntp                10.819788\n",
       "neurotherapeutics  10.819788\n",
       "trickster          10.819788\n",
       "pangya             10.819788\n",
       "omnigen            10.819788\n",
       "nutricosmetics     10.819788\n",
       "acreages           10.819788\n",
       "protide            10.819788\n",
       "characteristic     10.819788\n",
       "epicheck           10.819788\n",
       "careful            10.819788\n",
       "chemetics          10.819788\n",
       "photosensitive     10.819788\n",
       "suscription        10.819788\n",
       "nanopositioners    10.819788\n",
       "cauterization      10.819788\n",
       "miltiple           10.819788\n",
       "colli              10.819788\n",
       "quadrantanopia     10.819788\n",
       "...                      ...\n",
       "allows              2.749195\n",
       "information         2.744828\n",
       "helps               2.741410\n",
       "systems             2.741100\n",
       "web                 2.703819\n",
       "service             2.700687\n",
       "applications        2.694601\n",
       "business            2.694157\n",
       "manufacturer        2.558908\n",
       "enables             2.548496\n",
       "data                2.471961\n",
       "also                2.324841\n",
       "application         2.321370\n",
       "operator            2.307407\n",
       "technology          2.242912\n",
       "mobile              2.222214\n",
       "users               2.193920\n",
       "develops            2.162138\n",
       "management          2.041616\n",
       "based               2.014788\n",
       "software            1.951868\n",
       "products            1.943872\n",
       "online              1.682341\n",
       "developer           1.370470\n",
       "provides            1.294455\n",
       "platform            1.288343\n",
       "offers              1.277881\n",
       "services            1.135172\n",
       "provider            0.647687\n",
       "company             0.026754\n",
       "\n",
       "[49813 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the IDF vector\n",
    "idf_vector = pd.read_csv(\"../../data/100000_companies_with_description_idf_vector.csv\" ,header=None, \\\n",
    "                         names=['IDF'], index_col=0, encoding = \"ISO-8859-1\")\n",
    "idf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### End Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Run the next code no matter which option was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the idf values in a column vector\n",
    "idf_values = list(idf_vector.values)\n",
    "\n",
    "# Get the words in a column vector. The initial order mathes the \n",
    "# values in the idf_values_array\n",
    "idf_words = list(idf_vector.index.values)\n",
    "# Perform a reshape on the words array to get it in a better format\n",
    "\n",
    "idf_set = set(idf_words)\n",
    "idf_map = dict(zip(idf_words, idf_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# B) Creating the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next step will be to create an adjacency matrix to store all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>tx_industry</th>\n",
       "      <th>cb_category</th>\n",
       "      <th>tx_category</th>\n",
       "      <th>cb_desc</th>\n",
       "      <th>pb_desc</th>\n",
       "      <th>pb_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-in.com</td>\n",
       "      <td>Technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operator of an assertion-based verification co...</td>\n",
       "      <td>Automation/Workflow Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>012.net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telecom Operators</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provider of internet and international telepho...</td>\n",
       "      <td>Internet Service Providers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-2-3.tv</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>auctions|e-commerce|internet|shopping</td>\n",
       "      <td>Online Retail</td>\n",
       "      <td>1-2-3.tv is a multichannel auction house with ...</td>\n",
       "      <td>Operator of television broadcasting station th...</td>\n",
       "      <td>Broadcasting, Radio and Television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-2-social.de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Outsourcing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provider of social media marketing services. T...</td>\n",
       "      <td>Social Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-4.com</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>marketplace|mobile|software|transportation</td>\n",
       "      <td>Logistics Tech</td>\n",
       "      <td>10-4 is redefining the future of transportatio...</td>\n",
       "      <td>Provider of supply chain visibility technology...</td>\n",
       "      <td>Other Commercial Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          domain tx_industry                                 cb_category  \\\n",
       "0       0-in.com  Technology                                         NaN   \n",
       "1        012.net         NaN                                         NaN   \n",
       "2       1-2-3.tv    Consumer       auctions|e-commerce|internet|shopping   \n",
       "3  1-2-social.de         NaN                                         NaN   \n",
       "4       10-4.com    Consumer  marketplace|mobile|software|transportation   \n",
       "\n",
       "         tx_category                                            cb_desc  \\\n",
       "0     Semiconductors                                                NaN   \n",
       "1  Telecom Operators                                                NaN   \n",
       "2      Online Retail  1-2-3.tv is a multichannel auction house with ...   \n",
       "3        Outsourcing                                                NaN   \n",
       "4     Logistics Tech  10-4 is redefining the future of transportatio...   \n",
       "\n",
       "                                             pb_desc  \\\n",
       "0  Operator of an assertion-based verification co...   \n",
       "1  Provider of internet and international telepho...   \n",
       "2  Operator of television broadcasting station th...   \n",
       "3  Provider of social media marketing services. T...   \n",
       "4  Provider of supply chain visibility technology...   \n",
       "\n",
       "                          pb_category  \n",
       "0        Automation/Workflow Software  \n",
       "1          Internet Service Providers  \n",
       "2  Broadcasting, Radio and Television  \n",
       "3                      Social Content  \n",
       "4           Other Commercial Services  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "company_list = pd.read_csv('../../data/100000_companies_with_description.csv',  encoding = \"ISO-8859-1\", \\\n",
    "                           usecols=['domain', 'tx_industry', 'cb_category', 'tx_category', 'cb_desc',\\\n",
    "                                    'pb_desc', 'pb_category'])\n",
    "\n",
    "company_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ============ Stuck here, need to use a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_companies = company_list.shape[0]\n",
    "company_graph = np.empty((n_companies,n_companies))\n",
    "company_graph[:] = -1\n",
    "company_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we'll go through each word and see which companies have that word. \n",
    "We'll go through the top 1000 idf words, find the companies with those words, \n",
    "and then compare them to create the edge weight in the graph.  \n",
    "First, the following function creates a set of words out of the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Connor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Gets the words out of the labeled descriptions\n",
    "def get_words(df):\n",
    "    punctuation = '[^\\w\\s]'\n",
    "    txt = df.str.lower().str.replace(punctuation, ' ').str.cat(sep=' ')\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = nltk.tokenize.word_tokenize(txt)\n",
    "    return set(words) - stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we create a company_words_list, i.e. each company is associated with a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "company_words_list = [set()]*len(company_list)\n",
    "for i in range(len(company_list)):\n",
    "    start_index = 5\n",
    "    end_index = 7\n",
    "    company_words = get_words(company_list.iloc[i,start_index:end_index])\n",
    "    company_words_list[i] = company_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function goes through the list of companies and sees if a \n",
    "given word is in the description for each of the companies, \n",
    "returning a set of company indices with that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#given a target word and a pandas data frame of companies, \n",
    "# returns a list of companies whose descriptions contain the target word\n",
    "def get_companies(target_word, company_words_list):\n",
    "    candidate_set = set()\n",
    "    for i in range(len(company_words_list)):\n",
    "        company_description = company_words_list[i]\n",
    "        if target_word in company_description:\n",
    "            candidate_set.add(i)\n",
    "    return list(candidate_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function takes a candidate pair, and computes their weighted \n",
    "similarity by finding Jaccard similarity and then weighing it by the idf of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(company_index_1, company_index_2, company_words_list, idf_set, idf_map):\n",
    "    company_1 = company_words_list[company_index_1]\n",
    "    company_2 = company_words_list[company_index_2]\n",
    "    intersection = company_1 & company_2\n",
    "    union = company_1 | company_2\n",
    "    if len(union) == 0:\n",
    "        return 0\n",
    "    intersection_score = 0.0\n",
    "    union_score = 0.0\n",
    "    for word in union:\n",
    "        if word in idf_set:\n",
    "            word_score = idf_map[word][0]\n",
    "            union_score += word_score\n",
    "            if word in intersection:\n",
    "                intersection_score += word_score\n",
    "                \n",
    "    return intersection_score/union_score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we go through and construct the 3000x3000 adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_updated_elements = 0\n",
    "n_companies = len(company_list)\n",
    "cutoff = 0.1\n",
    "for i in range(n_companies):\n",
    "    for k in range((i+1), n_companies):\n",
    "        edge_weight = get_similarity(i, k, company_words_list, idf_set, idf_map)\n",
    "        if edge_weight >= cutoff:\n",
    "            company_graph[i][k] = edge_weight\n",
    "            company_graph[k][i] = edge_weight\n",
    "        \n",
    "#removing -1's and ensuring 1's along the diagonal\n",
    "\n",
    "np.fill_diagonal(company_graph, 1)\n",
    "company_graph[company_graph < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here I test different cutoffs for similarity scores to count as edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.96565522e-03   2.96565522e-03   2.96565522e-03   2.96565522e-03\n",
      "   2.96565522e-03   2.96565522e-03   2.96565522e-03   2.96565522e-03\n",
      "   2.96565522e-03   2.96565522e-03   1.90018895e-03   1.23107703e-03\n",
      "   8.17161276e-04   5.47960431e-04   3.82127376e-04   2.66310993e-04\n",
      "   1.89841058e-04   1.33822385e-04   9.80326776e-05   7.44692675e-05]\n"
     ]
    }
   ],
   "source": [
    "cutoff_sparsity = np.zeros(20)\n",
    "#cutoff_sparsity[i] gives the sparsity of the graph if similarity threshold is (i+1)*0.01\n",
    "n_possible_edges = float(company_graph.size - n_companies)\n",
    "for i in range(20):\n",
    "    cutoff_sparsity[i] = \\\n",
    "    (company_graph[company_graph > (i+1)*0.01].size - n_companies)/n_possible_edges\n",
    "print(cutoff_sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# C) Dot Product Heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'm going to perform a very simple heuristic to see how \n",
    "well the dot products predict similarity by seeing how they \n",
    "function on the company pairs with the highest textual similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mask = ~np.eye(company_graph.shape[0], dtype = bool)\n",
    "extremes = np.where((mask) & (company_graph > 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69330862,  0.90417218,  1.71394821,  1.61786232,  1.29913586,\n",
       "        1.2257629 ,  0.95953763,  0.95953763,  1.05905751,  0.85415079,\n",
       "        0.8595902 ,  1.05905751,  0.8595902 ,  0.94622351,  0.81868064,\n",
       "        1.2257629 ,  0.90417218,  0.85326372,  0.85415079,  1.71394821,\n",
       "        1.19521275,  0.94622351,  0.76862511,  0.81868064,  0.75469454,\n",
       "        1.08860499,  0.69330862,  0.75469454,  1.26190621,  1.61786232,\n",
       "        0.76862511,  1.08860499,  1.26190621,  0.85326372,  1.29913586,\n",
       "        1.19521275])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_products = np.zeros(len(extremes[0]))\n",
    "for i in range(len(dot_products)):\n",
    "    company_1 = extremes[0][i]\n",
    "    company_2 = extremes[1][i]\n",
    "    dot_products[i] = company_graph[company_1].dot(company_graph[company_2])\n",
    "dot_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll look at the descriptions for the 5 highest dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Score Pair 1\n",
      "--------------------------\n",
      "Company 1: csb-bk.com\n",
      "Company 2: cincinnatifederal.com\n",
      "\n",
      "Company 1 Description 1:\n",
      "Operator of a bank holding company. The company through its subsidiary provides personal and commercial banking services to its customers.\n",
      "\n",
      "Company 2 Description 1:\n",
      "Operator of a bank holding company. The company through its subsidiaries provides banking and banking and financial services to individual and corporate customers.\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "nan\n",
      "\n",
      "Company 2 Description 2:\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "Highest Score Pair 2\n",
      "--------------------------\n",
      "Company 1: groffr.com\n",
      "Company 2: pocketlistings.net\n",
      "\n",
      "Company 1 Description 1:\n",
      "nan\n",
      "\n",
      "Company 2 Description 1:\n",
      "nan\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "groffr.com is real estate based company.\n",
      "\n",
      "Company 2 Description 2:\n",
      "Off Market Real Estate Network\n",
      "\n",
      "\n",
      "\n",
      "Highest Score Pair 3\n",
      "--------------------------\n",
      "Company 1: ideaforge.co.in\n",
      "Company 2: canarddrones.com\n",
      "\n",
      "Company 1 Description 1:\n",
      "Developer of drones. The company develops and manufactures autonomous man-portable Unmanned Aerial Vehicles (UAVs).\n",
      "\n",
      "Company 2 Description 1:\n",
      "Manufacturer of drones and unmanned aerial vehicles. The company designs and develops automated calibration and verification of approaching lighting systems using unmanned aerial vehicles and drones.\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "ideaForge is an Indian company engaged in the development of unmanned aerial systems.\n",
      "\n",
      "Company 2 Description 2:\n",
      "CANARD allows fast calibration of NavAids by using fully automated, unmanned UAVs (drones).\n",
      "\n",
      "\n",
      "\n",
      "Highest Score Pair 4\n",
      "--------------------------\n",
      "Company 1: pocketlistings.net\n",
      "Company 2: groffr.com\n",
      "\n",
      "Company 1 Description 1:\n",
      "nan\n",
      "\n",
      "Company 2 Description 1:\n",
      "nan\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "Off Market Real Estate Network\n",
      "\n",
      "Company 2 Description 2:\n",
      "groffr.com is real estate based company.\n",
      "\n",
      "\n",
      "\n",
      "Highest Score Pair 5\n",
      "--------------------------\n",
      "Company 1: graduateland.com\n",
      "Company 2: aftercollege.com\n",
      "\n",
      "Company 1 Description 1:\n",
      "Provider of an online recruitment platform. The company's software creates a network that connects students or graduates, employers and universities via its job portal.\n",
      "\n",
      "Company 2 Description 1:\n",
      "Operator of an online career network providing employment services for college students and graduates. The company connects college students, alumni and employers through faculty and career networks at colleges and universities.\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "Graduateland offers a career network for students and recent graduates, enabling them to discover job opportunities.\n",
      "\n",
      "Company 2 Description 2:\n",
      "AfterCollege is a career network connecting college students and recent graduates with employers.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_scores = np.sort(dot_products)\n",
    "sorted_scores = sorted_scores[::-1]\n",
    "for i in range(5):\n",
    "    print('Highest Score Pair ' + str(i + 1))\n",
    "    print('--------------------------')\n",
    "    score = sorted_scores[2*i]\n",
    "    index = np.where(dot_products == score)\n",
    "    company_1 = extremes[0][index[0][0]]\n",
    "    company_2 = extremes[1][index[0][0]]\n",
    "    \n",
    "    print(\"Company 1: {}\\nCompany 2: {}\\n\".format(company_list.ix[company_1][0], company_list.ix[company_2][0]))\n",
    "\n",
    "    print(\"Company 1 Description 1:\\n{}\\n\".format(company_list.ix[company_1][5]))\n",
    "    print(\"Company 2 Description 1:\\n{}\".format(company_list.ix[company_2][5]))\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Company 1 Description 2:\\n{}\\n\".format(company_list.ix[company_1][6]))\n",
    "    print(\"Company 2 Description 2:\\n{}\".format(company_list.ix[company_2][6]))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's look at the 5 lowest dot products out of the pairs with the highest similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dot_products' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-876195fc1cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_products\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Lowest Score Pair '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dot_products' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_scores = np.sort(dot_products)\n",
    "for i in range(5):\n",
    "    print('Lowest Score Pair ' + str(i + 1))\n",
    "    print('--------------------------')\n",
    "    score = sorted_scores[2*i]\n",
    "    index = np.where(dot_products == score)\n",
    "    company_1 = extremes[0][index[0][0]]\n",
    "    company_2 = extremes[1][index[0][0]]\n",
    "    \n",
    "    print(\"Company 1: {}\\nCompany 2: {}\\n\".format(company_list.ix[company_1][0], company_list.ix[company_2][0]))\n",
    "\n",
    "    print(\"Company 1 Description 1:\\n{}\\n\".format(company_list.ix[company_1][5]))\n",
    "    print(\"Company 2 Description 1:\\n{}\".format(company_list.ix[company_2][5]))\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Company 1 Description 2:\\n{}\\n\".format(company_list.ix[company_1][6]))\n",
    "    print(\"Company 2 Description 2:\\n{}\".format(company_list.ix[company_2][6]))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    Both the top 5 and bottom 5 pairs had very high similarity for the most part, but perhaps that's just because I looked at such an extreme top end of textual similarity scores.  However, even here, the de-noising via dot product seems to work, especially in the case of homepage.com and jupviec.vn, which aren't as similar as their high textual similarity would seem to suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Connected Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll check how connected the graph is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the graph fully connected?: False\n",
      "Number of nodes in the graph: 3000\n",
      "Number of connected components: 494\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(company_list.index.values)\n",
    "for i in range(n_companies):\n",
    "    for k in range(i+1, n_companies):\n",
    "        if company_graph[i][k] != 0:\n",
    "            G.add_edge(i,k)\n",
    "            G[i][k]['weight'] = company_graph[i][k]\n",
    "\n",
    "print(\"Is the graph fully connected?: {}\".format(nx.is_connected(G)))\n",
    "print(\"Number of nodes in the graph: {}\".format(nx.number_of_nodes(G)))\n",
    "print(\"Number of connected components: {}\".format(nx.number_connected_components(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    The graph isn't connected, and has many connected components.  Let's see how nodes are distributed across these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2499,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_components = sorted(nx.connected_components(G), key = len, reverse = True)\n",
    "n_components = len(connected_components)\n",
    "elements_per_component = np.empty(n_components, dtype = int)\n",
    "for i in range(n_components):\n",
    "    elements_per_component[i] = len(connected_components[i])\n",
    "elements_per_component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    The majority of nodes are in one connected component, and the rest of the connected components are almost exclusively single nodes with no edges, with the exception of a few lone pairs.  Thus, while the graph isn't connected, for our intents/purposes we can treat it like it is (nodes with no edges won't provide any value in determining similarity), allowing the dot product/shared neighbors approach to potentially work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    Taking dot products seems to provide for some level of correction (albeit based on 1 pair that was marginally corrected by the low dot-product), even in the case of the pairs with the highest textual similarity.  It thus seems like a good denoising technique to use on the real graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
