{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### This notebook achieves the following objectives:\n",
    "\n",
    "    A) IDF-Vector Compute the IDF values for each word present in the corpus \n",
    "    of training samples (which in this case is the set of 3000 labeled companies)\n",
    "\n",
    "#### Note: B and C updated to attempt to use scipy sparse matrix format. (old code still in notebook at end)\n",
    "    B) Generate the graph. Compute the Jaccard similarity between \n",
    "    companies sharing similar high IDF words, and then populate the \n",
    "    graph where a node is a company and an edge is weighted by the \n",
    "    Jaccard similarity between the two companies (if calculated based on cutoff)\n",
    "\n",
    "    C) Run a simple test of taking two companies, placing all of \n",
    "    their weighted edges in a respective vector, then computing \n",
    "    the dot product between these two edges\n",
    "\n",
    "    Conclusion: We have found that the dot product method is \n",
    "    producing results in which the companies with the highest \n",
    "    dot products do in fact appear very similar. See part C for exact results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A) IDF Vector\n",
    "### To use an IDF vector, you have two options:\n",
    "\n",
    " #### Option 1: Use this code to generate a new IDF vector from words in the training dataset. This option has multiple cells to run\n",
    " \n",
    " #### Option 2: Use this code to read in a previously stored IDF vector. This option has one cell to run\n",
    "The currently implemented option 1 use case reads in the entire 650,000 companies from the raw data file and filters down to 100,000 companies which all have full pitch book descriptions\n",
    "             \n",
    " Note: Run either option 1 or option 2. If both are run, option 2 IDF vector will be used ebcause it will overwrite the variable set by option 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "class TfIdf:\n",
    "\n",
    "    \"\"\"Tf-idf class implementing http://en.wikipedia.org/wiki/Tf-idf.\n",
    "  \n",
    "     The library constructs an IDF corpus and stopword list either from\n",
    "     documents specified by the client, or by reading from input files.  It\n",
    "     computes IDF for a specified term based on the corpus, or generates\n",
    "     keywords ordered by tf-idf for a specified document.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_filename = None, stopword_filename = None,\n",
    "               DEFAULT_IDF = 1.5):\n",
    "        \"\"\"Initialize the idf dictionary.  \n",
    "    \n",
    "        If a corpus file is supplied, reads the idf dictionary from it, in the\n",
    "        format of:\n",
    "        # of total documents\n",
    "        term: # of documents containing the term\n",
    "\n",
    "        If a stopword file is specified, reads the stopword list from it, in\n",
    "        the format of one stopword per line.\n",
    "\n",
    "        The DEFAULT_IDF value is returned when a query term is not found in the\n",
    "        idf corpus.\n",
    "        \"\"\"\n",
    "        self.num_docs = 0\n",
    "        self.term_num_docs = {}     # term : num_docs_containing_term\n",
    "        self.stopwords = []\n",
    "        self.idf_default = DEFAULT_IDF\n",
    "\n",
    "        if corpus_filename:\n",
    "            corpus_file = open(corpus_filename, \"r\")\n",
    "\n",
    "          # Load number of documents.\n",
    "            line = corpus_file.readline()\n",
    "            self.num_docs = int(line.strip())\n",
    "\n",
    "          # Reads \"term:frequency\" from each subsequent line in the file.\n",
    "            for line in corpus_file:\n",
    "                tokens = line.split(\":\")\n",
    "                term = tokens[0].strip()\n",
    "                frequency = int(tokens[1].strip())\n",
    "                self.term_num_docs[term] = frequency\n",
    "\n",
    "        if stopword_filename:\n",
    "            stopword_file = open(stopword_filename, \"r\")\n",
    "            self.stopwords = [line.strip() for line in stopword_file]\n",
    "\n",
    "    def get_tokens(self, doc):\n",
    "        \"\"\"Break a string into tokens, preserving URL tags as an entire token.\n",
    "\n",
    "        This implementation does not preserve case.  \n",
    "        Clients may wish to override this behavior with their own tokenization.\n",
    "        \"\"\"\n",
    "        # Attempt 1 - Faster results (this one is faster than uncommented solution, but doesn't get rid of stop words)\n",
    "        #str_list = [word.lower().translate(str.maketrans(' ', ' ', string.punctuation)) for word in re.split('\\s|\\.|-|,',str(doc))]\n",
    "        \n",
    "        punctuation = '[^\\w\\s]'\n",
    "        doc = pd.Series(doc)\n",
    "        txt = doc.str.lower().str.replace(punctuation, ' ').str.cat(sep=' ')\n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        words = nltk.tokenize.word_tokenize(txt)\n",
    "        return set(words) - stopwords\n",
    "\n",
    "        \n",
    "    def add_input_document(self, input):\n",
    "        \"\"\"Add terms in the specified document to the idf dictionary.\"\"\"\n",
    "        self.num_docs += 1\n",
    "        words = set(self.get_tokens(input))\n",
    "        for word in words:\n",
    "            if word in self.term_num_docs:\n",
    "                self.term_num_docs[word] += 1\n",
    "            else:\n",
    "                self.term_num_docs[word] = 1\n",
    "\n",
    "    def save_corpus_to_file(self, idf_filename, stopword_filename,\n",
    "                          STOPWORD_PERCENTAGE_THRESHOLD = 0.01):\n",
    "        \"\"\"Save the idf dictionary and stopword list to the specified file.\"\"\"\n",
    "        output_file = open(idf_filename, \"w\")\n",
    "\n",
    "        output_file.write(str(self.num_docs) + \"\\n\")\n",
    "        for term, num_docs in self.term_num_docs.items():\n",
    "            output_file.write(term + \": \" + str(num_docs) + \"\\n\")\n",
    "\n",
    "        sorted_terms = sorted(self.term_num_docs.items(), key=itemgetter(1),\n",
    "                          reverse=True)\n",
    "        stopword_file = open(stopword_filename, \"w\")\n",
    "        for term, num_docs in sorted_terms:\n",
    "            if num_docs < STOPWORD_PERCENTAGE_THRESHOLD * self.num_docs:\n",
    "                break\n",
    "\n",
    "            stopword_file.write(term + \"\\n\")\n",
    "\n",
    "    def get_num_docs(self):\n",
    "        \"\"\"Return the total number of documents in the IDF corpus.\"\"\"\n",
    "        return self.num_docs\n",
    "\n",
    "    def get_idf(self, term):\n",
    "        \"\"\"Retrieve the IDF for the specified term. \n",
    "    \n",
    "        This is computed by taking the logarithm of ( \n",
    "        (number of documents in corpus) divided by (number of documents\n",
    "        containing this term) ).\n",
    "        \"\"\"\n",
    "        if term in self.stopwords:\n",
    "            return 0\n",
    "\n",
    "        if not term in self.term_num_docs:\n",
    "            return self.idf_default\n",
    "\n",
    "        return math.log(float(1 + self.get_num_docs()) / (1 + self.term_num_docs[term]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Now read in the full raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#################### Full Data #########################\n",
    "training_categories_df = pd.read_csv(\"../../data/raw_data_fixed.csv\",  encoding = \"ISO-8859-1\", usecols=['domain'\\\n",
    ", 'tx_industry', 'cb_category', 'tx_category', 'cb_desc', 'pb_desc', 'pb_category'])\n",
    "\n",
    "#mydoclist = training_categories_df.ix[0:,'pb_desc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Filter out companies that do not have a pitch book provided description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 166406 companies that have a full pitch book provided description\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter out all companies that don't have a pitch book provided description\n",
    "subset_df_pb_desc = training_categories_df.ix[training_categories_df['pb_desc'].notnull()]\n",
    "\n",
    "\n",
    "# Now print out some info\n",
    "len_of_df = subset_df_pb_desc.shape[0]\n",
    "print(\"There are {} companies that have a full pitch book provided description\".format(len_of_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of final dataframe to use for building the graph: 100000\n"
     ]
    }
   ],
   "source": [
    "# Cut this down to 100,000 companies using the head function\n",
    "subset_df_pb_desc = subset_df_pb_desc.head(100000)\n",
    "print(\"Size of final dataframe to use for building the graph: {}\".format(subset_df_pb_desc.shape[0]))\n",
    "#subset_df_pb_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below line if you need to write the filtered companies to a csv\n",
    "subset_df_pb_desc.to_csv(\"../../data/100000_companies_with_description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "mydoclist = subset_df_pb_desc['pb_desc'].values\n",
    "print(len(mydoclist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### This code computes the actual IDF vector and can take a couple minutes to run with 100,000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idfcalc = TfIdf()\n",
    "for entry in mydoclist:\n",
    "    idfcalc.add_input_document(entry)\n",
    "#print(idfcalc.term_num_docs)\n",
    "\n",
    "idf_vec = []\n",
    "term_vec = []\n",
    "for term in idfcalc.term_num_docs:\n",
    "    idf = idfcalc.get_idf(term)\n",
    "    idf_vec.append(idf)\n",
    "    term_vec.append(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Convert the IDF vector a pandas Series and sort by IDF value (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dioxin               10.819788\n",
      "spiegelmers          10.819788\n",
      "nanopositioning      10.819788\n",
      "schlumberger         10.819788\n",
      "handelsblad          10.819788\n",
      "nrc                  10.819788\n",
      "gassification        10.819788\n",
      "pyro                 10.819788\n",
      "tme                  10.819788\n",
      "ntag                 10.819788\n",
      "latticed             10.819788\n",
      "ntp                  10.819788\n",
      "neurotherapeutics    10.819788\n",
      "trickster            10.819788\n",
      "pangya               10.819788\n",
      "omnigen              10.819788\n",
      "nutricosmetics       10.819788\n",
      "acreages             10.819788\n",
      "protide              10.819788\n",
      "characteristic       10.819788\n",
      "epicheck             10.819788\n",
      "careful              10.819788\n",
      "chemetics            10.819788\n",
      "photosensitive       10.819788\n",
      "suscription          10.819788\n",
      "nanopositioners      10.819788\n",
      "cauterization        10.819788\n",
      "miltiple             10.819788\n",
      "colli                10.819788\n",
      "quadrantanopia       10.819788\n",
      "                       ...    \n",
      "allows                2.749195\n",
      "information           2.744828\n",
      "helps                 2.741410\n",
      "systems               2.741100\n",
      "web                   2.703819\n",
      "service               2.700687\n",
      "applications          2.694601\n",
      "business              2.694157\n",
      "manufacturer          2.558908\n",
      "enables               2.548496\n",
      "data                  2.471961\n",
      "also                  2.324841\n",
      "application           2.321370\n",
      "operator              2.307407\n",
      "technology            2.242912\n",
      "mobile                2.222214\n",
      "users                 2.193920\n",
      "develops              2.162138\n",
      "management            2.041616\n",
      "based                 2.014788\n",
      "software              1.951868\n",
      "products              1.943872\n",
      "online                1.682341\n",
      "developer             1.370470\n",
      "provides              1.294455\n",
      "platform              1.288343\n",
      "offers                1.277881\n",
      "services              1.135172\n",
      "provider              0.647687\n",
      "company               0.026754\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "idf_vector = pd.Series(idf_vec, index=term_vec)\n",
    "\n",
    "sorted_idf_vector = idf_vector.sort_values(ascending=False)\n",
    "idf_vector = sorted_idf_vector\n",
    "print(idf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Uncomment the below line to save the IDF vector to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This code saves the IDF vector to a file\n",
    "# idf_vector.to_csv(\"../../data/100000_companies_with_description_idf_vector.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### End Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Option 2 (skip this if option 1 was utilized, else proceed to read an IDF vector from a csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cranberries</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>siteÃ­s</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maintainable</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fermented</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raspberry</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pomegranate</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>juice</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chinesespeaking</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loop</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affairs</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>towels</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carpets</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quilts</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cloth</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedding</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>board</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commercialuse</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>member</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysing</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>executing</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temporary</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confined</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaffolding</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structural</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eworkshops</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micronized</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>continuing</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>econference</th>\n",
       "      <td>7.313554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator</th>\n",
       "      <td>2.438356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enables</th>\n",
       "      <td>2.368346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile</th>\n",
       "      <td>2.350709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>2.350709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>2.340274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>companys</th>\n",
       "      <td>2.323121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>2.216741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application</th>\n",
       "      <td>2.207608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>users</th>\n",
       "      <td>2.087807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>develops</th>\n",
       "      <td>2.082445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>2.077112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>2.025287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online</th>\n",
       "      <td>1.764478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>services</th>\n",
       "      <td>1.724434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provides</th>\n",
       "      <td>1.551502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offers</th>\n",
       "      <td>1.540556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>1.537451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>1.511435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1.506914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developer</th>\n",
       "      <td>1.490508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>1.433021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platform</th>\n",
       "      <td>1.299839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0.965289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provider</th>\n",
       "      <td>0.873405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.842754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.827393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.384037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.376240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.255656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.255656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7602 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      IDF\n",
       "3                7.313554\n",
       "cranberries      7.313554\n",
       "siteÃ­s          7.313554\n",
       "maintainable     7.313554\n",
       "fermented        7.313554\n",
       "raspberry        7.313554\n",
       "pomegranate      7.313554\n",
       "juice            7.313554\n",
       "chinesespeaking  7.313554\n",
       "loop             7.313554\n",
       "affairs          7.313554\n",
       "towels           7.313554\n",
       "carpets          7.313554\n",
       "quilts           7.313554\n",
       "cloth            7.313554\n",
       "bedding          7.313554\n",
       "board            7.313554\n",
       "commercialuse    7.313554\n",
       "member           7.313554\n",
       "analysing        7.313554\n",
       "executing        7.313554\n",
       "temporary        7.313554\n",
       "confined         7.313554\n",
       "scaffolding      7.313554\n",
       "structural       7.313554\n",
       "321              7.313554\n",
       "eworkshops       7.313554\n",
       "micronized       7.313554\n",
       "continuing       7.313554\n",
       "econference      7.313554\n",
       "...                   ...\n",
       "operator         2.438356\n",
       "enables          2.368346\n",
       "mobile           2.350709\n",
       "on               2.350709\n",
       "software         2.340274\n",
       "companys         2.323121\n",
       "which            2.216741\n",
       "application      2.207608\n",
       "users            2.087807\n",
       "develops         2.082445\n",
       "their            2.077112\n",
       "with             2.025287\n",
       "online           1.764478\n",
       "services         1.724434\n",
       "provides         1.551502\n",
       "offers           1.540556\n",
       "that             1.537451\n",
       "an               1.511435\n",
       "NaN              1.506914\n",
       "developer        1.490508\n",
       "in               1.433021\n",
       "platform         1.299839\n",
       "for              0.965289\n",
       "provider         0.873405\n",
       "to               0.842754\n",
       "a                0.827393\n",
       "company          0.384037\n",
       "and              0.376240\n",
       "of               0.255656\n",
       "the              0.255656\n",
       "\n",
       "[7602 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Read in the IDF vector  - 100000 companies\n",
    "#idf_vector = pd.read_csv(\"../../data/100000_companies_with_description_idf_vector.csv\" ,header=None, \\\n",
    "#                         names=['IDF'], index_col=0, encoding = \"ISO-8859-1\")\n",
    "\n",
    "### Read the IDF vector - 3000 companies\n",
    "idf_vector = pd.read_csv(\"../../data/training_idf_vector.csv\" ,header=None, names=['IDF'], index_col=0, encoding = \"ISO-8859-1\")\n",
    "idf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### End Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Run the next code no matter which option was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the idf values in a column vector\n",
    "idf_values = list(idf_vector.values)\n",
    "\n",
    "# Get the words in a column vector. The initial order mathes the \n",
    "# values in the idf_values_array\n",
    "idf_words = list(idf_vector.index.values)\n",
    "# Perform a reshape on the words array to get it in a better format\n",
    "\n",
    "idf_set = set(idf_words)\n",
    "idf_map = dict(zip(idf_words, idf_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# B) Creating the Graph  (Updated to try to use Sparse matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next step will be to create an adjacency matrix to store all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>rs_category</th>\n",
       "      <th>tx_industry</th>\n",
       "      <th>cb_category</th>\n",
       "      <th>tx_category</th>\n",
       "      <th>pb_desc</th>\n",
       "      <th>cb_desc</th>\n",
       "      <th>pb_industry</th>\n",
       "      <th>pb_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conferencecloud.co</td>\n",
       "      <td>Business Communication Application</td>\n",
       "      <td>NaN</td>\n",
       "      <td>information technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provider of an online conferencing platform. T...</td>\n",
       "      <td>ConferenceCloud provides state-of-the-art live...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Social/Platform Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>terminus.com</td>\n",
       "      <td>Marketing Software and Service</td>\n",
       "      <td>AdTech,AdTech,Enterprise Applications</td>\n",
       "      <td>advertising|advertising platforms|b2b</td>\n",
       "      <td>AdTech,MarketingTech,SaaS</td>\n",
       "      <td>Developer of a B2B advertising platform. The c...</td>\n",
       "      <td>Terminus is a platform that seamlessly integra...</td>\n",
       "      <td>AdTech, Marketing Tech</td>\n",
       "      <td>Business/Productivity Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>galileoprocessing.com</td>\n",
       "      <td>Payment Application</td>\n",
       "      <td>Fintech</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Payment Cards</td>\n",
       "      <td>Provider of payment processing services. The c...</td>\n",
       "      <td>Next generation card processing platform</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Other Financial Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pubble.co</td>\n",
       "      <td>Business Communication Application</td>\n",
       "      <td>NaN</td>\n",
       "      <td>digital media|education|software</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operator of a community engagement platform. T...</td>\n",
       "      <td>Pubble is a messaging platform that simplifies...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Software Development Applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cajo.fi</td>\n",
       "      <td>Printing Technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Developer of a stainless steel colour patterni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Machinery (B2B)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  domain                         rs_category  \\\n",
       "0     conferencecloud.co  Business Communication Application   \n",
       "1           terminus.com      Marketing Software and Service   \n",
       "2  galileoprocessing.com                 Payment Application   \n",
       "3              pubble.co  Business Communication Application   \n",
       "4                cajo.fi                 Printing Technology   \n",
       "\n",
       "                             tx_industry  \\\n",
       "0                                    NaN   \n",
       "1  AdTech,AdTech,Enterprise Applications   \n",
       "2                                Fintech   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "                             cb_category                tx_category  \\\n",
       "0                 information technology                        NaN   \n",
       "1  advertising|advertising platforms|b2b  AdTech,MarketingTech,SaaS   \n",
       "2                                    NaN              Payment Cards   \n",
       "3       digital media|education|software                        NaN   \n",
       "4                                    NaN                        NaN   \n",
       "\n",
       "                                             pb_desc  \\\n",
       "0  Provider of an online conferencing platform. T...   \n",
       "1  Developer of a B2B advertising platform. The c...   \n",
       "2  Provider of payment processing services. The c...   \n",
       "3  Operator of a community engagement platform. T...   \n",
       "4  Developer of a stainless steel colour patterni...   \n",
       "\n",
       "                                             cb_desc             pb_industry  \\\n",
       "0  ConferenceCloud provides state-of-the-art live...                     NaN   \n",
       "1  Terminus is a platform that seamlessly integra...  AdTech, Marketing Tech   \n",
       "2           Next generation card processing platform                 FinTech   \n",
       "3  Pubble is a messaging platform that simplifies...                     NaN   \n",
       "4                                                NaN           Manufacturing   \n",
       "\n",
       "                         pb_category  \n",
       "0           Social/Platform Software  \n",
       "1     Business/Productivity Software  \n",
       "2           Other Financial Services  \n",
       "3  Software Development Applications  \n",
       "4                    Machinery (B2B)  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "##### 100,0000 companies\n",
    "#company_list = pd.read_csv('../../data/100000_companies_with_description.csv',  encoding = \"ISO-8859-1\", \\\n",
    "#                           usecols=['domain', 'tx_industry', 'cb_category', 'tx_category', 'cb_desc',\\\n",
    "#                                    'pb_desc', 'pb_category'])\n",
    "\n",
    "##### 3000 companies\n",
    "company_list = pd.read_csv('../../data/category_training_labeled_fixed.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "company_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ============ Attempt to use a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3000x3000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in LInked List format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "n_companies = company_list.shape[0]\n",
    "company_graph = lil_matrix((n_companies,n_companies))\n",
    "#company_graph[:] = -1\n",
    "company_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Connor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Gets the words out of the labeled descriptions\n",
    "def get_words(df):\n",
    "    punctuation = '[^\\w\\s]'\n",
    "    txt = df.str.lower().str.replace(punctuation, ' ').str.cat(sep=' ')\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = nltk.tokenize.word_tokenize(txt)\n",
    "    return set(words) - stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    }
   ],
   "source": [
    "company_words_list = [set()]*len(company_list)\n",
    "for i in range(len(company_list)):\n",
    "    #start_index = 5\n",
    "    #end_index = 7\n",
    "    if i%10000 is 0:\n",
    "        print(\"iteration {}\".format(i))\n",
    "    company_words = get_words(company_list.iloc[i,5:6])\n",
    "    company_words_list[i] = company_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#given a target word and a pandas data frame of companies, \n",
    "# returns a list of companies whose descriptions contain the target word\n",
    "def get_companies(target_word, company_words_list):\n",
    "    candidate_set = set()\n",
    "    for i in range(len(company_words_list)):\n",
    "        company_description = company_words_list[i]\n",
    "        if target_word in company_description:\n",
    "            candidate_set.add(i)\n",
    "    return list(candidate_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(company_index_1, company_index_2, company_words_list, idf_set, idf_map):\n",
    "    company_1 = company_words_list[company_index_1]\n",
    "    company_2 = company_words_list[company_index_2]\n",
    "    intersection = company_1 & company_2\n",
    "    union = company_1 | company_2\n",
    "    if len(union) == 0:\n",
    "        return 0\n",
    "    intersection_score = 0.0\n",
    "    union_score = 0.0\n",
    "    for word in union:\n",
    "        if word in idf_set:\n",
    "            word_score = idf_map[word][0]\n",
    "            union_score += word_score\n",
    "            if word in intersection:\n",
    "                intersection_score += word_score\n",
    "                \n",
    "    return intersection_score/union_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is an attempt at buliding the graph into a scipy sparse lil_matrix format, with a slight modification to try and help scale the code\n",
    "   It appeared that once we found a pair of companies that met the similarity cutoff, we added the value to the (i,k) position, and to the (k,i) position. From how I understood it, we are then looking at the same position over again later on in the iterations and computing the same value twice. I therefore attempted to add a dictionary look up table, and I added the (k,i) position, convereted to a string, as a key. I then check if that is in the dictionary every kth iteration, and if it is, I skip that iteration (since we already calculated and stored the value in both positions). Still think it might be a little buggy because it doesn't seem to speed up like I think it should once the count of (n_companies - len(visited_table)) is zero. I added a break statement to break the outer loop once this occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The lil_matrix way to access an index is via \"lil_matrix[i,k]\" instead of \"matrix[i][k]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ith iteration: 0 out of 2976\n",
      "ith iteration: 10 out of 2925\n",
      "ith iteration: 20 out of 2846\n",
      "ith iteration: 30 out of 2741\n",
      "ith iteration: 40 out of 2682\n",
      "ith iteration: 50 out of 2605\n",
      "ith iteration: 60 out of 2446\n",
      "ith iteration: 70 out of 2357\n",
      "ith iteration: 80 out of 2261\n",
      "ith iteration: 90 out of 2220\n",
      "ith iteration: 100 out of 2206\n",
      "ith iteration: 110 out of 2144\n",
      "ith iteration: 120 out of 2019\n",
      "ith iteration: 130 out of 1929\n",
      "ith iteration: 140 out of 1885\n",
      "ith iteration: 150 out of 1846\n",
      "ith iteration: 160 out of 1831\n",
      "ith iteration: 170 out of 1764\n",
      "ith iteration: 180 out of 1657\n",
      "ith iteration: 190 out of 1518\n",
      "ith iteration: 200 out of 1340\n",
      "ith iteration: 210 out of 1263\n",
      "ith iteration: 220 out of 1226\n",
      "ith iteration: 230 out of 1114\n",
      "ith iteration: 240 out of 1001\n",
      "ith iteration: 250 out of 880\n",
      "ith iteration: 260 out of 653\n",
      "ith iteration: 270 out of 556\n",
      "ith iteration: 280 out of 421\n",
      "ith iteration: 290 out of 320\n",
      "ith iteration: 300 out of 300\n",
      "ith iteration: 310 out of 222\n",
      "ith iteration: 320 out of 157\n",
      "ith iteration: 330 out of 94\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "n_updated_elements = 0\n",
    "n_companies = len(company_list)\n",
    "cutoff = 0.1\n",
    "\n",
    "#Seems like this code is redundant, in that it \n",
    "visited_table = {}\n",
    "\n",
    "for i in range(n_companies):\n",
    "    if n_companies - len(visited_table) <= 0:\n",
    "        print(\"Finished\")\n",
    "        break\n",
    "    for k in range((i+1), n_companies):\n",
    "        if str(k)+str(i) in visited_table or str(i)+str(k) in visited_table:\n",
    "            continue\n",
    "        else:\n",
    "            edge_weight = get_similarity(i, k, company_words_list, idf_set, idf_map)\n",
    "            if edge_weight >= cutoff:\n",
    "                company_graph[i,k] = edge_weight\n",
    "                company_graph[k,i] = edge_weight\n",
    "            # Add these companies to graph and skip later on to save time\n",
    "                visited_table[str(k)+str(i)] = 1\n",
    "    if i%10 is 0:\n",
    "        print(\"ith iteration: {} out of {}\".format(i,n_companies - len(visited_table)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not sure about this code below either, but the setting values < 0 to 0 appears to run. Not sure if it works as intended though because the sparse matrix format doesn't work with alll normal numpy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.fill_diagonal(company_graph, 1)\n",
    "company_graph[company_graph < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Didn't make any changes to the next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          0.54254302  0.23924474  0.0583174\n",
      " -0.07194073 -0.15941683 -0.21510516 -0.25740918 -0.28680688 -0.30544933\n",
      " -0.31883365]\n"
     ]
    }
   ],
   "source": [
    "cutoff_sparsity = np.zeros(20)\n",
    "#cutoff_sparsity[i] gives the sparsity of the graph if similarity threshold is (i+1)*0.01\n",
    "n_possible_edges = float(company_graph.size - n_companies)\n",
    "for i in range(20):\n",
    "    cutoff_sparsity[i] = \\\n",
    "    (company_graph[company_graph > (i+1)*0.01].size - n_companies)/n_possible_edges\n",
    "print(cutoff_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(company_list['domain'].values)\n",
    "#G.add_nodes_from([x in range(n_companies)])\n",
    "for i in range(n_companies):\n",
    "    for k in range(i, n_companies):\n",
    "        if company_graph[i,k] != 0:         # changed this indexing operation to [i,k] from [i][k]\n",
    "            G.add_edge(i,k)\n",
    "            G[i][k]['weight'] = company_graph[i,k]\n",
    "\n",
    "\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(200, 200), dpi=80)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    #pos = nx.spring_layout(graph)\n",
    "    pos = nx.spring_layout(graph,k=0.9,iterations=5)\n",
    "    nx.draw_networkx_nodes(graph,pos,node_color='g',node_size = 30,linewidths=0)\n",
    "    nx.draw_networkx_edges(graph,pos,edge_color='b')\n",
    "    nx.draw_networkx_labels(graph,pos,label_size = 30)\n",
    "\n",
    "    cut = 1.00\n",
    "    xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    plt.xlim(0, xmax)\n",
    "    plt.ylim(0, ymax)\n",
    "\n",
    "    plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "    pylab.close()\n",
    "    del fig\n",
    "\n",
    "\n",
    "save_graph(G,\"3K_graph_sparse_matrix_version.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) New Dot Product Heuristic Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now convert the lil_matrix to a csr_matrix format and do the dot products\n",
    "\n",
    "- why? from scipy documentation, csr_matrix format is more efficient for doing matrix multiplication, while the lil_matrix format is better for fast indexing/building the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3000x3000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11368 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "company_graph_csr = company_graph.tocsr()\n",
    "company_graph_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having trouble getting the code from here on out working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'bool' and 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-f3e305181ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Not sure what this is for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_graph_csr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mextremes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompany_graph_csr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'bool' and 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "# Not sure what this is for, so having trouble getting it to work with csr_matrix format\n",
    "mask = ~np.eye(company_graph_csr.shape[0], dtype = bool)\n",
    "extremes = np.where((mask) & (company_graph_csr > 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extremes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-22355705cb1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdot_products\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_products\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcompany_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextremes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcompany_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextremes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdot_products\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompany_graph_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompany_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_graph_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompany_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extremes' is not defined"
     ]
    }
   ],
   "source": [
    "dot_products = np.zeros(100)\n",
    "for i in range(len(dot_products)):\n",
    "    company_1 = extremes[0][i]\n",
    "    company_2 = extremes[1][i]\n",
    "    dot_products[i] = company_graph_csr[company_1].dot(company_graph_csr[company_2])\n",
    "dot_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_scores = np.sort(dot_products)\n",
    "sorted_scores = sorted_scores[::-1]\n",
    "for i in range(5):\n",
    "    print('Highest Score Pair ' + str(i + 1))\n",
    "    print('--------------------------')\n",
    "    score = sorted_scores[2*i]\n",
    "    index = np.where(dot_products == score)\n",
    "    company_1 = extremes[0][index[0][0]]\n",
    "    company_2 = extremes[1][index[0][0]]\n",
    "    \n",
    "    print(\"Company 1: {}\\nCompany 2: {}\\n\".format(company_list.ix[company_1][0], company_list.ix[company_2][0]))\n",
    "\n",
    "    print(\"Company 1 Description 1:\\n{}\\n\".format(company_list.ix[company_1][5]))\n",
    "    print(\"Company 2 Description 1:\\n{}\".format(company_list.ix[company_2][5]))\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Company 1 Description 2:\\n{}\\n\".format(company_list.ix[company_1][6]))\n",
    "    print(\"Company 2 Description 2:\\n{}\".format(company_list.ix[company_2][6]))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_scores = np.sort(dot_products)\n",
    "for i in range(5):\n",
    "    print('Lowest Score Pair ' + str(i + 1))\n",
    "    print('--------------------------')\n",
    "    score = sorted_scores[2*i]\n",
    "    index = np.where(dot_products == score)\n",
    "    company_1 = extremes[0][index[0][0]]\n",
    "    company_2 = extremes[1][index[0][0]]\n",
    "    \n",
    "    print(\"Company 1: {}\\nCompany 2: {}\\n\".format(company_list.ix[company_1][0], company_list.ix[company_2][0]))\n",
    "\n",
    "    print(\"Company 1 Description 1:\\n{}\\n\".format(company_list.ix[company_1][5]))\n",
    "    print(\"Company 2 Description 1:\\n{}\".format(company_list.ix[company_2][5]))\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Company 1 Description 2:\\n{}\\n\".format(company_list.ix[company_1][6]))\n",
    "    print(\"Company 2 Description 2:\\n{}\".format(company_list.ix[company_2][6]))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== End of new code/ Start of old graph building code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_companies = company_list.shape[0]\n",
    "company_graph = np.empty((n_companies,n_companies))\n",
    "company_graph[:] = -1\n",
    "company_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we'll go through each word and see which companies have that word. \n",
    "We'll go through the top 1000 idf words, find the companies with those words, \n",
    "and then compare them to create the edge weight in the graph.  \n",
    "First, the following function creates a set of words out of the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Connor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Gets the words out of the labeled descriptions\n",
    "def get_words(df):\n",
    "    punctuation = '[^\\w\\s]'\n",
    "    txt = df.str.lower().str.replace(punctuation, ' ').str.cat(sep=' ')\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = nltk.tokenize.word_tokenize(txt)\n",
    "    return set(words) - stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we create a company_words_list, i.e. each company is associated with a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "company_words_list = [set()]*len(company_list)\n",
    "for i in range(len(company_list)):\n",
    "    start_index = 5\n",
    "    end_index = 7\n",
    "    company_words = get_words(company_list.iloc[i,start_index:end_index])\n",
    "    company_words_list[i] = company_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function goes through the list of companies and sees if a \n",
    "given word is in the description for each of the companies, \n",
    "returning a set of company indices with that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#given a target word and a pandas data frame of companies, \n",
    "# returns a list of companies whose descriptions contain the target word\n",
    "def get_companies(target_word, company_words_list):\n",
    "    candidate_set = set()\n",
    "    for i in range(len(company_words_list)):\n",
    "        company_description = company_words_list[i]\n",
    "        if target_word in company_description:\n",
    "            candidate_set.add(i)\n",
    "    return list(candidate_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function takes a candidate pair, and computes their weighted \n",
    "similarity by finding Jaccard similarity and then weighing it by the idf of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(company_index_1, company_index_2, company_words_list, idf_set, idf_map):\n",
    "    company_1 = company_words_list[company_index_1]\n",
    "    company_2 = company_words_list[company_index_2]\n",
    "    intersection = company_1 & company_2\n",
    "    union = company_1 | company_2\n",
    "    if len(union) == 0:\n",
    "        return 0\n",
    "    intersection_score = 0.0\n",
    "    union_score = 0.0\n",
    "    for word in union:\n",
    "        if word in idf_set:\n",
    "            word_score = idf_map[word][0]\n",
    "            union_score += word_score\n",
    "            if word in intersection:\n",
    "                intersection_score += word_score\n",
    "                \n",
    "    return intersection_score/union_score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we go through and construct the 3000x3000 adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_updated_elements = 0\n",
    "n_companies = len(company_list)\n",
    "cutoff = 0.1\n",
    "for i in range(n_companies):\n",
    "    for k in range((i+1), n_companies):\n",
    "        edge_weight = get_similarity(i, k, company_words_list, idf_set, idf_map)\n",
    "        if edge_weight >= cutoff:\n",
    "            company_graph[i][k] = edge_weight\n",
    "            company_graph[k][i] = edge_weight\n",
    "        \n",
    "#removing -1's and ensuring 1's along the diagonal\n",
    "\n",
    "np.fill_diagonal(company_graph, 1)\n",
    "company_graph[company_graph < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here I test different cutoffs for similarity scores to count as edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.96565522e-03   2.96565522e-03   2.96565522e-03   2.96565522e-03\n",
      "   2.96565522e-03   2.96565522e-03   2.96565522e-03   2.96565522e-03\n",
      "   2.96565522e-03   2.96565522e-03   1.90018895e-03   1.23107703e-03\n",
      "   8.17161276e-04   5.47960431e-04   3.82127376e-04   2.66310993e-04\n",
      "   1.89841058e-04   1.33822385e-04   9.80326776e-05   7.44692675e-05]\n"
     ]
    }
   ],
   "source": [
    "cutoff_sparsity = np.zeros(20)\n",
    "#cutoff_sparsity[i] gives the sparsity of the graph if similarity threshold is (i+1)*0.01\n",
    "n_possible_edges = float(company_graph.size - n_companies)\n",
    "for i in range(20):\n",
    "    cutoff_sparsity[i] = \\\n",
    "    (company_graph[company_graph > (i+1)*0.01].size - n_companies)/n_possible_edges\n",
    "print(cutoff_sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dot Product Heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'm going to perform a very simple heuristic to see how \n",
    "well the dot products predict similarity by seeing how they \n",
    "function on the company pairs with the highest textual similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mask = ~np.eye(company_graph.shape[0], dtype = bool)\n",
    "extremes = np.where((mask) & (company_graph > 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69330862,  0.90417218,  1.71394821,  1.61786232,  1.29913586,\n",
       "        1.2257629 ,  0.95953763,  0.95953763,  1.05905751,  0.85415079,\n",
       "        0.8595902 ,  1.05905751,  0.8595902 ,  0.94622351,  0.81868064,\n",
       "        1.2257629 ,  0.90417218,  0.85326372,  0.85415079,  1.71394821,\n",
       "        1.19521275,  0.94622351,  0.76862511,  0.81868064,  0.75469454,\n",
       "        1.08860499,  0.69330862,  0.75469454,  1.26190621,  1.61786232,\n",
       "        0.76862511,  1.08860499,  1.26190621,  0.85326372,  1.29913586,\n",
       "        1.19521275])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_products = np.zeros(len(extremes[0]))\n",
    "for i in range(len(dot_products)):\n",
    "    company_1 = extremes[0][i]\n",
    "    company_2 = extremes[1][i]\n",
    "    dot_products[i] = company_graph[company_1].dot(company_graph[company_2])\n",
    "dot_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll look at the descriptions for the 5 highest dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Score Pair 1\n",
      "--------------------------\n",
      "Company 1: csb-bk.com\n",
      "Company 2: cincinnatifederal.com\n",
      "\n",
      "Company 1 Description 1:\n",
      "Operator of a bank holding company. The company through its subsidiary provides personal and commercial banking services to its customers.\n",
      "\n",
      "Company 2 Description 1:\n",
      "Operator of a bank holding company. The company through its subsidiaries provides banking and banking and financial services to individual and corporate customers.\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "nan\n",
      "\n",
      "Company 2 Description 2:\n",
      "nan\n",
      "\n",
      "\n",
      "\n",
      "Highest Score Pair 2\n",
      "--------------------------\n",
      "Company 1: groffr.com\n",
      "Company 2: pocketlistings.net\n",
      "\n",
      "Company 1 Description 1:\n",
      "nan\n",
      "\n",
      "Company 2 Description 1:\n",
      "nan\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "groffr.com is real estate based company.\n",
      "\n",
      "Company 2 Description 2:\n",
      "Off Market Real Estate Network\n",
      "\n",
      "\n",
      "\n",
      "Highest Score Pair 3\n",
      "--------------------------\n",
      "Company 1: ideaforge.co.in\n",
      "Company 2: canarddrones.com\n",
      "\n",
      "Company 1 Description 1:\n",
      "Developer of drones. The company develops and manufactures autonomous man-portable Unmanned Aerial Vehicles (UAVs).\n",
      "\n",
      "Company 2 Description 1:\n",
      "Manufacturer of drones and unmanned aerial vehicles. The company designs and develops automated calibration and verification of approaching lighting systems using unmanned aerial vehicles and drones.\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "ideaForge is an Indian company engaged in the development of unmanned aerial systems.\n",
      "\n",
      "Company 2 Description 2:\n",
      "CANARD allows fast calibration of NavAids by using fully automated, unmanned UAVs (drones).\n",
      "\n",
      "\n",
      "\n",
      "Highest Score Pair 4\n",
      "--------------------------\n",
      "Company 1: pocketlistings.net\n",
      "Company 2: groffr.com\n",
      "\n",
      "Company 1 Description 1:\n",
      "nan\n",
      "\n",
      "Company 2 Description 1:\n",
      "nan\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "Off Market Real Estate Network\n",
      "\n",
      "Company 2 Description 2:\n",
      "groffr.com is real estate based company.\n",
      "\n",
      "\n",
      "\n",
      "Highest Score Pair 5\n",
      "--------------------------\n",
      "Company 1: graduateland.com\n",
      "Company 2: aftercollege.com\n",
      "\n",
      "Company 1 Description 1:\n",
      "Provider of an online recruitment platform. The company's software creates a network that connects students or graduates, employers and universities via its job portal.\n",
      "\n",
      "Company 2 Description 1:\n",
      "Operator of an online career network providing employment services for college students and graduates. The company connects college students, alumni and employers through faculty and career networks at colleges and universities.\n",
      "\n",
      "\n",
      "Company 1 Description 2:\n",
      "Graduateland offers a career network for students and recent graduates, enabling them to discover job opportunities.\n",
      "\n",
      "Company 2 Description 2:\n",
      "AfterCollege is a career network connecting college students and recent graduates with employers.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_scores = np.sort(dot_products)\n",
    "sorted_scores = sorted_scores[::-1]\n",
    "for i in range(5):\n",
    "    print('Highest Score Pair ' + str(i + 1))\n",
    "    print('--------------------------')\n",
    "    score = sorted_scores[2*i]\n",
    "    index = np.where(dot_products == score)\n",
    "    company_1 = extremes[0][index[0][0]]\n",
    "    company_2 = extremes[1][index[0][0]]\n",
    "    \n",
    "    print(\"Company 1: {}\\nCompany 2: {}\\n\".format(company_list.ix[company_1][0], company_list.ix[company_2][0]))\n",
    "\n",
    "    print(\"Company 1 Description 1:\\n{}\\n\".format(company_list.ix[company_1][5]))\n",
    "    print(\"Company 2 Description 1:\\n{}\".format(company_list.ix[company_2][5]))\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Company 1 Description 2:\\n{}\\n\".format(company_list.ix[company_1][6]))\n",
    "    print(\"Company 2 Description 2:\\n{}\".format(company_list.ix[company_2][6]))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's look at the 5 lowest dot products out of the pairs with the highest similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dot_products' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-876195fc1cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_products\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Lowest Score Pair '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dot_products' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_scores = np.sort(dot_products)\n",
    "for i in range(5):\n",
    "    print('Lowest Score Pair ' + str(i + 1))\n",
    "    print('--------------------------')\n",
    "    score = sorted_scores[2*i]\n",
    "    index = np.where(dot_products == score)\n",
    "    company_1 = extremes[0][index[0][0]]\n",
    "    company_2 = extremes[1][index[0][0]]\n",
    "    \n",
    "    print(\"Company 1: {}\\nCompany 2: {}\\n\".format(company_list.ix[company_1][0], company_list.ix[company_2][0]))\n",
    "\n",
    "    print(\"Company 1 Description 1:\\n{}\\n\".format(company_list.ix[company_1][5]))\n",
    "    print(\"Company 2 Description 1:\\n{}\".format(company_list.ix[company_2][5]))\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Company 1 Description 2:\\n{}\\n\".format(company_list.ix[company_1][6]))\n",
    "    print(\"Company 2 Description 2:\\n{}\".format(company_list.ix[company_2][6]))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    Both the top 5 and bottom 5 pairs had very high similarity for the most part, but perhaps that's just because I looked at such an extreme top end of textual similarity scores.  However, even here, the de-noising via dot product seems to work, especially in the case of homepage.com and jupviec.vn, which aren't as similar as their high textual similarity would seem to suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Connected Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll check how connected the graph is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the graph fully connected?: False\n",
      "Number of nodes in the graph: 3000\n",
      "Number of connected components: 494\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(company_list.index.values)\n",
    "for i in range(n_companies):\n",
    "    for k in range(i+1, n_companies):\n",
    "        if company_graph[i][k] != 0:\n",
    "            G.add_edge(i,k)\n",
    "            G[i][k]['weight'] = company_graph[i][k]\n",
    "\n",
    "print(\"Is the graph fully connected?: {}\".format(nx.is_connected(G)))\n",
    "print(\"Number of nodes in the graph: {}\".format(nx.number_of_nodes(G)))\n",
    "print(\"Number of connected components: {}\".format(nx.number_connected_components(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    The graph isn't connected, and has many connected components.  Let's see how nodes are distributed across these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2499,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_components = sorted(nx.connected_components(G), key = len, reverse = True)\n",
    "n_components = len(connected_components)\n",
    "elements_per_component = np.empty(n_components, dtype = int)\n",
    "for i in range(n_components):\n",
    "    elements_per_component[i] = len(connected_components[i])\n",
    "elements_per_component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    The majority of nodes are in one connected component, and the rest of the connected components are almost exclusively single nodes with no edges, with the exception of a few lone pairs.  Thus, while the graph isn't connected, for our intents/purposes we can treat it like it is (nodes with no edges won't provide any value in determining similarity), allowing the dot product/shared neighbors approach to potentially work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    Taking dot products seems to provide for some level of correction (albeit based on 1 pair that was marginally corrected by the low dot-product), even in the case of the pairs with the highest textual similarity.  It thus seems like a good denoising technique to use on the real graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
