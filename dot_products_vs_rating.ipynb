{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "### This notebook achieves the following objectives:\n",
      "\n",
      "    A) IDF-Vector Compute the IDF values for each word present in the corpus \n",
      "    of training samples (which in this case is the set of 3000 labeled companies)\n",
      "\n",
      "    B) Generate the graph. Compute the Jaccard similarity between \n",
      "    companies sharing similar high IDF words, and then populate the \n",
      "    graph where a node is a company and an edge is weighted by the \n",
      "    Jaccard similarity between the two companies (if calculated based on cutoff)\n",
      "\n",
      "    C) Run a simple test of taking two companies, placing all of \n",
      "    their weighted edges in a respective vector, then computing \n",
      "    the dot product between these two edges\n",
      "\n",
      "    Conclusion: We have found that the dot product method is \n",
      "    producing results in which the companies with the highest \n",
      "    dot products do in fact appear very similar. See part C for exact results\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "# A) IDF Vector\n",
      "### To use an IDF vector, you have two options:\n",
      "\n",
      " #### Option 1: Use this code to generate a new IDF vector from words in the training dataset. This option has multiple cells to run\n",
      " \n",
      " #### Option 2: Use this code to read in a previously stored IDF vector. This option has one cell to run\n",
      "The currently implemented option 2 use case reads in the entire 650,000 companies from the raw data file and filters down to 100,000 companies which all have full pitch book descriptions\n",
      "             \n",
      " Note: Run either option 1 or option 2. If both are run, option 2 IDF vector will be used ebcause it will overwrite the variable set by option 1\n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "  ----------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Option 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import math\n",
      "import re\n",
      "from operator import itemgetter\n",
      "import string\n",
      "import nltk\n",
      "\n",
      "class TfIdf:\n",
      "\n",
      "    \"\"\"Tf-idf class implementing http://en.wikipedia.org/wiki/Tf-idf.\n",
      "  \n",
      "     The library constructs an IDF corpus and stopword list either from\n",
      "     documents specified by the client, or by reading from input files.  It\n",
      "     computes IDF for a specified term based on the corpus, or generates\n",
      "     keywords ordered by tf-idf for a specified document.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, corpus_filename = None, stopword_filename = None,\n",
      "               DEFAULT_IDF = 1.5):\n",
      "        \"\"\"Initialize the idf dictionary.  \n",
      "    \n",
      "        If a corpus file is supplied, reads the idf dictionary from it, in the\n",
      "        format of:\n",
      "        # of total documents\n",
      "        term: # of documents containing the term\n",
      "\n",
      "        If a stopword file is specified, reads the stopword list from it, in\n",
      "        the format of one stopword per line.\n",
      "\n",
      "        The DEFAULT_IDF value is returned when a query term is not found in the\n",
      "        idf corpus.\n",
      "        \"\"\"\n",
      "        self.num_docs = 0\n",
      "        self.term_num_docs = {}     # term : num_docs_containing_term\n",
      "        self.stopwords = []\n",
      "        self.idf_default = DEFAULT_IDF\n",
      "\n",
      "        if corpus_filename:\n",
      "            corpus_file = open(corpus_filename, \"r\")\n",
      "\n",
      "          # Load number of documents.\n",
      "            line = corpus_file.readline()\n",
      "            self.num_docs = int(line.strip())\n",
      "\n",
      "          # Reads \"term:frequency\" from each subsequent line in the file.\n",
      "            for line in corpus_file:\n",
      "                tokens = line.split(\":\")\n",
      "                term = tokens[0].strip()\n",
      "                frequency = int(tokens[1].strip())\n",
      "                self.term_num_docs[term] = frequency\n",
      "\n",
      "        if stopword_filename:\n",
      "            stopword_file = open(stopword_filename, \"r\")\n",
      "            self.stopwords = [line.strip() for line in stopword_file]\n",
      "\n",
      "    def get_tokens(self, doc):\n",
      "        \"\"\"Break a string into tokens, preserving URL tags as an entire token.\n",
      "\n",
      "        This implementation does not preserve case.  \n",
      "        Clients may wish to override this behavior with their own tokenization.\n",
      "        \"\"\"\n",
      "        # Attempt 1 - Faster results (this one is faster than uncommented solution, but doesn't get rid of stop words)\n",
      "        #str_list = [word.lower().translate(str.maketrans(' ', ' ', string.punctuation)) for word in re.split('\\s|\\.|-|,',str(doc))]\n",
      "        \n",
      "        punctuation = '[^\\w\\s]'\n",
      "        doc = pd.Series(doc)\n",
      "        txt = doc.str.lower().str.replace(punctuation, ' ').str.cat(sep=' ')\n",
      "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
      "        words = nltk.tokenize.word_tokenize(txt)\n",
      "        return set(words) - stopwords\n",
      "\n",
      "        \n",
      "    def add_input_document(self, input):\n",
      "        \"\"\"Add terms in the specified document to the idf dictionary.\"\"\"\n",
      "        self.num_docs += 1\n",
      "        words = set(self.get_tokens(input))\n",
      "        for word in words:\n",
      "            if word in self.term_num_docs:\n",
      "                self.term_num_docs[word] += 1\n",
      "            else:\n",
      "                self.term_num_docs[word] = 1\n",
      "\n",
      "    def save_corpus_to_file(self, idf_filename, stopword_filename,\n",
      "                          STOPWORD_PERCENTAGE_THRESHOLD = 0.01):\n",
      "        \"\"\"Save the idf dictionary and stopword list to the specified file.\"\"\"\n",
      "        output_file = open(idf_filename, \"w\")\n",
      "\n",
      "        output_file.write(str(self.num_docs) + \"\\n\")\n",
      "        for term, num_docs in self.term_num_docs.items():\n",
      "            output_file.write(term + \": \" + str(num_docs) + \"\\n\")\n",
      "\n",
      "        sorted_terms = sorted(self.term_num_docs.items(), key=itemgetter(1),\n",
      "                          reverse=True)\n",
      "        stopword_file = open(stopword_filename, \"w\")\n",
      "        for term, num_docs in sorted_terms:\n",
      "            if num_docs < STOPWORD_PERCENTAGE_THRESHOLD * self.num_docs:\n",
      "                break\n",
      "\n",
      "            stopword_file.write(term + \"\\n\")\n",
      "\n",
      "    def get_num_docs(self):\n",
      "        \"\"\"Return the total number of documents in the IDF corpus.\"\"\"\n",
      "        return self.num_docs\n",
      "\n",
      "    def get_idf(self, term):\n",
      "        \"\"\"Retrieve the IDF for the specified term. \n",
      "    \n",
      "        This is computed by taking the logarithm of ( \n",
      "        (number of documents in corpus) divided by (number of documents\n",
      "        containing this term) ).\n",
      "        \"\"\"\n",
      "        if term in self.stopwords:\n",
      "            return 0\n",
      "\n",
      "        if not term in self.term_num_docs:\n",
      "            return self.idf_default\n",
      "\n",
      "        return math.log(float(1 + self.get_num_docs()) / (1 + self.term_num_docs[term]))"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Now read in the full raw dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "#################### Full Data #########################\n",
      "training_categories_df = pd.read_csv(\"/users/Meeranster/CS341/data/raw_data_fixed.csv\",  encoding = \"ISO-8859-1\", usecols=['domain'\\\n",
      ", 'tx_industry', 'cb_category', 'tx_category', 'cb_desc', 'pb_desc', 'pb_category'])\n",
      "\n",
      "#mydoclist = training_categories_df.ix[0:,'pb_desc'].values"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Applications/Canopy.app/appdata/canopy-1.5.3.3103.macosx-x86_64/Canopy.app/Contents/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2741: DtypeWarning: Columns (1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
        "  interactivity=interactivity, compiler=compiler)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Filter out companies that do not have a pitch book provided description"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Filter out all companies that don't have a pitch book provided description\n",
      "subset_df_pb_desc = training_categories_df.ix[training_categories_df['pb_desc'].notnull()]\n",
      "\n",
      "\n",
      "# Now print out some info\n",
      "len_of_df = subset_df_pb_desc.shape[0]\n",
      "print(\"There are {} companies that have a full pitch book provided description\".format(len_of_df))\n"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 166406 companies that have a full pitch book provided description\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Uncomment below line if you need to write the filtered companies to a csv\n",
      "#subset_df_pb_desc.to_csv(\"../../data/100000_companies_with_description.csv\")"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Cut this down to 100,000 companies using the head function\n",
      "subset_df_pb_desc = subset_df_pb_desc.head(10000)\n",
      "company_list = subset_df_pb_desc\n",
      "print(\"Size of final dataframe to use for building the graph: {}\".format(subset_df_pb_desc.shape[0]))\n",
      "#subset_df_pb_desc"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Size of final dataframe to use for building the graph: 10000\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Adding missing companies from the pairs dataset to the graph."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, filter all the rated pairs for those with both PitchBook descriptions present."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rated_pairs =  pd.read_csv(\"/users/Meeranster/CS341/data/company_pairs_training.csv\",  encoding = \"ISO-8859-1\")\n",
      "training_pairs = rated_pairs.ix[rated_pairs['pb_desc1'].notnull()]\n",
      "training_pairs = training_pairs.ix[rated_pairs['pb_desc2'].notnull()]\n",
      "company_list.iloc[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "domain                                                  0-in.com\n",
        "tx_industry                                           Technology\n",
        "cb_category                                                  NaN\n",
        "tx_category                                       Semiconductors\n",
        "cb_desc                                                      NaN\n",
        "pb_desc        Operator of an assertion-based verification co...\n",
        "pb_category                         Automation/Workflow Software\n",
        "Name: 2, dtype: object"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, go through the list of companies in these pairs and see if they are in our initial dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "company_set = set(company_list['domain'].values)\n",
      "#domains 1 and 2 are indexes 0 and 1 in the panda series\n",
      "n_pairs = training_pairs.shape[0]\n",
      "new_companies = []\n",
      "newest = 0\n",
      "company1_fields = ['domain1', 'tx_industry1', 'cb_category1', 'tx_category1',\n",
      "                   'cb_desc1', 'pb_desc1', 'pb_category1']\n",
      "company2_fields = ['domain2', 'tx_industry2', 'cb_category2', 'tx_category2',\n",
      "                   'cb_desc2', 'pb_desc2', 'pb_category2']\n",
      "new_fields = ['domain', 'tx_industry', 'cb_category', 'tx_category',\n",
      "                   'cb_desc', 'pb_desc', 'pb_category']\n",
      "for i in range(n_pairs):\n",
      "    company1 = training_pairs.iloc[i]['domain1']\n",
      "    company2 = training_pairs.iloc[i]['domain2']\n",
      "    if company1 not in company_set:\n",
      "        new_companies.append(dict(zip(new_fields, training_pairs.iloc[i][company1_fields])))\n",
      "        company_set.add(company1)\n",
      "        newest += 1\n",
      "    if company2 not in company_set:\n",
      "        new_companies.append(dict(zip(new_fields, training_pairs.iloc[i][company2_fields])))\n",
      "        newest += 1\n",
      "        company_set.add(company2)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.DataFrame(new_companies)\n",
      "company_list = company_list.append(df)\n",
      "company_list.reset_index(inplace = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create doclist for idf calculations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mydoclist = company_list['pb_desc'].values\n",
      "print(len(mydoclist))\n"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11153\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "This code computes the actual IDF vector and can take a couple minutes to run with 100,000 samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "idfcalc = TfIdf()\n",
      "for entry in mydoclist:\n",
      "    idfcalc.add_input_document(entry)\n",
      "#print(idfcalc.term_num_docs)\n",
      "\n",
      "idf_vec = []\n",
      "term_vec = []\n",
      "for term in idfcalc.term_num_docs:\n",
      "    idf = idfcalc.get_idf(term)\n",
      "    idf_vec.append(idf)\n",
      "    term_vec.append(term)"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Convert the IDF vector a pandas Series and sort by IDF value (in descending order)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idf_vector = pd.Series(idf_vec, index=term_vec)\n",
      "\n",
      "sorted_idf_vector = idf_vector.sort_values(ascending=False)\n",
      "idf_vector = sorted_idf_vector\n",
      "print(idf_vector)"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "continence      8.626406\n",
        "tripod          8.626406\n",
        "netflix         8.626406\n",
        "voltmeters      8.626406\n",
        "nanodiamond     8.626406\n",
        "worldnet        8.626406\n",
        "kibbles         8.626406\n",
        "endorsed        8.626406\n",
        "acre            8.626406\n",
        "arthrits        8.626406\n",
        "storytellers    8.626406\n",
        "replies         8.626406\n",
        "wrinkles        8.626406\n",
        "hemorrhage      8.626406\n",
        "melbourne       8.626406\n",
        "railcars        8.626406\n",
        "fluorometric    8.626406\n",
        "arranged        8.626406\n",
        "401ks           8.626406\n",
        "enodeb          8.626406\n",
        "drills          8.626406\n",
        "duration        8.626406\n",
        "suitcase        8.626406\n",
        "loxapine        8.626406\n",
        "spiritus        8.626406\n",
        "tricorder       8.626406\n",
        "evaluated       8.626406\n",
        "chronographs    8.626406\n",
        "resolving       8.626406\n",
        "pixelpipe       8.626406\n",
        "                  ...   \n",
        "development     2.755698\n",
        "business        2.727880\n",
        "designed        2.712903\n",
        "web             2.706169\n",
        "enabling        2.690190\n",
        "allows          2.637445\n",
        "helps           2.636193\n",
        "service         2.628711\n",
        "applications    2.490841\n",
        "also            2.450539\n",
        "enables         2.442257\n",
        "operator        2.418823\n",
        "data            2.342272\n",
        "application     2.278142\n",
        "users           2.198301\n",
        "management      2.078904\n",
        "mobile          2.077471\n",
        "technology      2.059031\n",
        "develops        2.035419\n",
        "products        2.028579\n",
        "based           1.900773\n",
        "software        1.849899\n",
        "online          1.595991\n",
        "services        1.333729\n",
        "offers          1.331689\n",
        "provides        1.331010\n",
        "developer       1.218785\n",
        "platform        1.171686\n",
        "provider        0.689389\n",
        "company         0.056052\n",
        "dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Uncomment the below line to save the IDF vector to a file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code saves the IDF vector to a file\n",
      "# idf_vector.to_csv(\"../../data/100000_companies_with_description_idf_vector.csv\")"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "End Option 1"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Option 2 (skip this if option 1 was utilized, else proceed to read an IDF vector from a csv file)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Read in the IDF vector\n",
      "idf_vector = pd.read_csv(\"../../data/100000_companies_with_description_idf_vector.csv\" ,header=None, \\\n",
      "                         names=['IDF'], index_col=0, encoding = \"ISO-8859-1\")\n",
      "idf_vector"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "html": [
        "<div>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>IDF</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>dioxin</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>spiegelmers</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>nanopositioning</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>schlumberger</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>handelsblad</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>nrc</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>gassification</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pyro</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>tme</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ntag</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>latticed</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ntp</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>neurotherapeutics</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>trickster</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pangya</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>omnigen</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>nutricosmetics</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>acreages</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>protide</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>characteristic</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>epicheck</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>careful</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>chemetics</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>photosensitive</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>suscription</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>nanopositioners</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>cauterization</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>miltiple</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>colli</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>quadrantanopia</th>\n",
        "      <td>10.819788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>allows</th>\n",
        "      <td>2.749195</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>information</th>\n",
        "      <td>2.744828</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helps</th>\n",
        "      <td>2.741410</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>systems</th>\n",
        "      <td>2.741100</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>web</th>\n",
        "      <td>2.703819</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>service</th>\n",
        "      <td>2.700687</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>applications</th>\n",
        "      <td>2.694601</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>business</th>\n",
        "      <td>2.694157</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>manufacturer</th>\n",
        "      <td>2.558908</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>enables</th>\n",
        "      <td>2.548496</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>data</th>\n",
        "      <td>2.471961</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>also</th>\n",
        "      <td>2.324841</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>application</th>\n",
        "      <td>2.321370</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>operator</th>\n",
        "      <td>2.307407</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>technology</th>\n",
        "      <td>2.242912</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mobile</th>\n",
        "      <td>2.222214</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>users</th>\n",
        "      <td>2.193920</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>develops</th>\n",
        "      <td>2.162138</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>management</th>\n",
        "      <td>2.041616</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>based</th>\n",
        "      <td>2.014788</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>software</th>\n",
        "      <td>1.951868</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>products</th>\n",
        "      <td>1.943872</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>online</th>\n",
        "      <td>1.682341</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>developer</th>\n",
        "      <td>1.370470</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>provides</th>\n",
        "      <td>1.294455</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>platform</th>\n",
        "      <td>1.288343</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>offers</th>\n",
        "      <td>1.277881</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>services</th>\n",
        "      <td>1.135172</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>provider</th>\n",
        "      <td>0.647687</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>company</th>\n",
        "      <td>0.026754</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>49813 rows \u00d7 1 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "                         IDF\n",
        "dioxin             10.819788\n",
        "spiegelmers        10.819788\n",
        "nanopositioning    10.819788\n",
        "schlumberger       10.819788\n",
        "handelsblad        10.819788\n",
        "nrc                10.819788\n",
        "gassification      10.819788\n",
        "pyro               10.819788\n",
        "tme                10.819788\n",
        "ntag               10.819788\n",
        "latticed           10.819788\n",
        "ntp                10.819788\n",
        "neurotherapeutics  10.819788\n",
        "trickster          10.819788\n",
        "pangya             10.819788\n",
        "omnigen            10.819788\n",
        "nutricosmetics     10.819788\n",
        "acreages           10.819788\n",
        "protide            10.819788\n",
        "characteristic     10.819788\n",
        "epicheck           10.819788\n",
        "careful            10.819788\n",
        "chemetics          10.819788\n",
        "photosensitive     10.819788\n",
        "suscription        10.819788\n",
        "nanopositioners    10.819788\n",
        "cauterization      10.819788\n",
        "miltiple           10.819788\n",
        "colli              10.819788\n",
        "quadrantanopia     10.819788\n",
        "...                      ...\n",
        "allows              2.749195\n",
        "information         2.744828\n",
        "helps               2.741410\n",
        "systems             2.741100\n",
        "web                 2.703819\n",
        "service             2.700687\n",
        "applications        2.694601\n",
        "business            2.694157\n",
        "manufacturer        2.558908\n",
        "enables             2.548496\n",
        "data                2.471961\n",
        "also                2.324841\n",
        "application         2.321370\n",
        "operator            2.307407\n",
        "technology          2.242912\n",
        "mobile              2.222214\n",
        "users               2.193920\n",
        "develops            2.162138\n",
        "management          2.041616\n",
        "based               2.014788\n",
        "software            1.951868\n",
        "products            1.943872\n",
        "online              1.682341\n",
        "developer           1.370470\n",
        "provides            1.294455\n",
        "platform            1.288343\n",
        "offers              1.277881\n",
        "services            1.135172\n",
        "provider            0.647687\n",
        "company             0.026754\n",
        "\n",
        "[49813 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "End Option 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Run the next code no matter which option was used"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Get the idf values in a column vector\n",
      "idf_values = list(idf_vector.values)\n",
      "\n",
      "# Get the words in a column vector. The initial order mathes the \n",
      "# values in the idf_values_array\n",
      "idf_words = list(idf_vector.index.values)\n",
      "# Perform a reshape on the words array to get it in a better format\n",
      "\n",
      "idf_set = set(idf_words)\n",
      "idf_map = dict(zip(idf_words, idf_values))\n"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "B) Creating the Graph"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "The next step will be to create an adjacency matrix to store all these values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "#Uncomment if you did Option 2 instead of Option 1\n",
      "# company_list = pd.read_csv('../../data/100000_companies_with_description.csv',  encoding = \"ISO-8859-1\", \\\n",
      "#                            usecols=['domain', 'tx_industry', 'cb_category', 'tx_category', 'cb_desc',\\\n",
      "#                                     'pb_desc', 'pb_category'])\n",
      "\n",
      "company_list.head()"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>index</th>\n",
        "      <th>cb_category</th>\n",
        "      <th>cb_desc</th>\n",
        "      <th>domain</th>\n",
        "      <th>pb_category</th>\n",
        "      <th>pb_desc</th>\n",
        "      <th>tx_category</th>\n",
        "      <th>tx_industry</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>2</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0-in.com</td>\n",
        "      <td>Automation/Workflow Software</td>\n",
        "      <td>Operator of an assertion-based verification co...</td>\n",
        "      <td>Semiconductors</td>\n",
        "      <td>Technology</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>16</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>012.net</td>\n",
        "      <td>Internet Service Providers</td>\n",
        "      <td>Provider of internet and international telepho...</td>\n",
        "      <td>Telecom Operators</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>34</td>\n",
        "      <td>auctions|e-commerce|internet|shopping</td>\n",
        "      <td>1-2-3.tv is a multichannel auction house with ...</td>\n",
        "      <td>1-2-3.tv</td>\n",
        "      <td>Broadcasting, Radio and Television</td>\n",
        "      <td>Operator of television broadcasting station th...</td>\n",
        "      <td>Online Retail</td>\n",
        "      <td>Consumer</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>35</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1-2-social.de</td>\n",
        "      <td>Social Content</td>\n",
        "      <td>Provider of social media marketing services. T...</td>\n",
        "      <td>Outsourcing</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>47</td>\n",
        "      <td>marketplace|mobile|software|transportation</td>\n",
        "      <td>10-4 is redefining the future of transportatio...</td>\n",
        "      <td>10-4.com</td>\n",
        "      <td>Other Commercial Services</td>\n",
        "      <td>Provider of supply chain visibility technology...</td>\n",
        "      <td>Logistics Tech</td>\n",
        "      <td>Consumer</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "   index                                 cb_category  \\\n",
        "0      2                                         NaN   \n",
        "1     16                                         NaN   \n",
        "2     34       auctions|e-commerce|internet|shopping   \n",
        "3     35                                         NaN   \n",
        "4     47  marketplace|mobile|software|transportation   \n",
        "\n",
        "                                             cb_desc         domain  \\\n",
        "0                                                NaN       0-in.com   \n",
        "1                                                NaN        012.net   \n",
        "2  1-2-3.tv is a multichannel auction house with ...       1-2-3.tv   \n",
        "3                                                NaN  1-2-social.de   \n",
        "4  10-4 is redefining the future of transportatio...       10-4.com   \n",
        "\n",
        "                          pb_category  \\\n",
        "0        Automation/Workflow Software   \n",
        "1          Internet Service Providers   \n",
        "2  Broadcasting, Radio and Television   \n",
        "3                      Social Content   \n",
        "4           Other Commercial Services   \n",
        "\n",
        "                                             pb_desc        tx_category  \\\n",
        "0  Operator of an assertion-based verification co...     Semiconductors   \n",
        "1  Provider of internet and international telepho...  Telecom Operators   \n",
        "2  Operator of television broadcasting station th...      Online Retail   \n",
        "3  Provider of social media marketing services. T...        Outsourcing   \n",
        "4  Provider of supply chain visibility technology...     Logistics Tech   \n",
        "\n",
        "  tx_industry  \n",
        "0  Technology  \n",
        "1         NaN  \n",
        "2    Consumer  \n",
        "3         NaN  \n",
        "4    Consumer  "
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "============ Stuck here, need to use a sparse matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "n_companies = company_list.shape[0]\n",
      "company_graph = np.zeros((n_companies,n_companies))\n",
      "company_graph[:] = 0\n",
      "n_companies"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "11153"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Next, we'll go through each word and see which companies have that word. \n",
      "We'll go through the top 1000 idf words, find the companies with those words, \n",
      "and then compare them to create the edge weight in the graph.  \n",
      "First, the following function creates a set of words out of the description"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "\n",
      "#Gets the words out of the labeled descriptions\n",
      "def get_words(df):\n",
      "    punctuation = '[^\\w\\s]'\n",
      "    txt = df.str.lower().str.replace(punctuation, ' ').str.cat(sep=' ')\n",
      "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
      "    words = nltk.tokenize.word_tokenize(txt)\n",
      "    return set(words) - stopwords\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package punkt to\n",
        "[nltk_data]     /Users/Meeranster/nltk_data...\n",
        "[nltk_data]   Package punkt is already up-to-date!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Now, we create a company_words_list, i.e. each company is associated with a set of words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "company_words_list = [set()]*len(company_list)\n",
      "for i in range(len(company_list)):\n",
      "    pb_desc_index = 5\n",
      "    company_words = get_words(company_list.iloc[i,pb_desc_index:(pb_desc_index + 1)])\n",
      "    company_words_list[i] = company_words\n"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(company_words_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "11153"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "This function goes through the list of companies and sees if a \n",
      "given word is in the description for each of the companies, \n",
      "returning a set of company indices with that word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#given a target word and a pandas data frame of companies, \n",
      "# returns a list of companies whose descriptions contain the target word\n",
      "def get_companies(target_word, company_words_list):\n",
      "    candidate_set = set()\n",
      "    for i in range(len(company_words_list)):\n",
      "        company_description = company_words_list[i]\n",
      "        if target_word in company_description:\n",
      "            candidate_set.add(i)\n",
      "    return list(candidate_set)\n",
      "        "
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "This function takes a candidate pair, and computes their weighted \n",
      "similarity by finding Jaccard similarity and then weighing it by the idf of the words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_similarity(company_index_1, company_index_2, company_words_list, idf_set, idf_map):\n",
      "    company_1 = company_words_list[company_index_1]\n",
      "    company_2 = company_words_list[company_index_2]\n",
      "    intersection = company_1 & company_2\n",
      "    union = company_1 | company_2\n",
      "    if len(union) == 0:\n",
      "        return 0\n",
      "    intersection_score = 0.0\n",
      "    union_score = 0.0\n",
      "    for word in union:\n",
      "        if word in idf_set:\n",
      "            word_score = idf_map[word]\n",
      "            union_score += word_score\n",
      "            if word in intersection:\n",
      "                intersection_score += word_score\n",
      "                \n",
      "    return intersection_score/union_score\n",
      "        "
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Now, we go through and construct the adjacency matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size(company_graph[company_graph > 0])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_updated_elements = 0\n",
      "n_companies = len(company_list)\n",
      "cutoff = 0.05\n",
      "for i in range(n_companies):\n",
      "    for k in range((i+1), n_companies):\n",
      "        edge_weight = get_similarity(i, k, company_words_list, idf_set, idf_map)\n",
      "        if edge_weight >= cutoff:\n",
      "            company_graph[i][k] = edge_weight\n",
      "            company_graph[k][i] = edge_weight\n",
      "        \n",
      "#removing -1's and ensuring 1's along the diagonal\n",
      "\n",
      "np.fill_diagonal(company_graph, 1)\n",
      "company_graph[company_graph < 0] = 0"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What's the graph's sparsity?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "float(size(company_graph[company_graph > 0]) - n_companies)/(n_companies*n_companies - n_companies)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "0.03265064272970671"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Here I test different cutoffs for similarity scores to count as edges."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cutoff_sparsity = np.zeros(20)\n",
      "#cutoff_sparsity[i] gives the sparsity of the graph if similarity threshold is (i+1)*0.01\n",
      "n_possible_edges = float(company_graph.size - n_companies)\n",
      "for i in range(20):\n",
      "    cutoff_sparsity[i] = \\\n",
      "    (company_graph[company_graph > (i+1)*0.01].size - n_companies)/n_possible_edges\n",
      "print(cutoff_sparsity)"
     ],
     "language": "python",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  2.96565522e-03   2.96565522e-03   2.96565522e-03   2.96565522e-03\n",
        "   2.96565522e-03   2.96565522e-03   2.96565522e-03   2.96565522e-03\n",
        "   2.96565522e-03   2.96565522e-03   1.90018895e-03   1.23107703e-03\n",
        "   8.17161276e-04   5.47960431e-04   3.82127376e-04   2.66310993e-04\n",
        "   1.89841058e-04   1.33822385e-04   9.80326776e-05   7.44692675e-05]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "C) Comparison to Rated Similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "deletable": true,
      "editable": true
     },
     "source": [
      "Computing dot products over training pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 91,
       "text": [
        "array([ 0.0164598 ,  0.        ,  0.50668986, ...,  0.        ,\n",
        "        0.06104581,  0.        ])"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "domain_index_map = dict(zip(company_list['domain'].values, company_list.index.values))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(company_list.shape[0]):\n",
      "    company_name = company_list.ix[i]['domain']\n",
      "    if domain_index_map[company_name] != i:\n",
      "        print('bug')\n",
      "        break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = np.zeros(n_pairs)\n",
      "for i in range(n_pairs):\n",
      "    company1_index = domain_index_map[training_pairs.iloc[i]['domain1']]\n",
      "    company2_index = domain_index_map[training_pairs.iloc[i]['domain2']]\n",
      "    dot_product = company_graph[company1_index].dot(company_graph[company2_index])\n",
      "    scores[i] = dot_product\n",
      "#training_pairs['dot_products'] = pd.Series(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll see how that matches up with the known scores."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "ratings = (training_pairs['rating'].reset_index())\n",
      "ratings['scores'] = pd.Series(scores)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "clf = SVC()\n",
      "dot_scores = np.array(ratings['scores'])\n",
      "pipeline_dot_scores = dot_scores.reshape((n_pairs, 1))\n",
      "known_ratings = np.array(ratings['rating'])\n",
      "clf.fit(pipeline_dot_scores[0:n_pairs/2], known_ratings[0:n_pairs/2])\n",
      "predicted = clf.predict(pipeline_dot_scores[n_pairs/2:])\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy = predicted - known_ratings[n_pairs/2:]\n",
      "correctness = float(len(accuracy[accuracy == 0]))/len(accuracy)\n",
      "underestimation = float(len(accuracy[accuracy > 0]))/len(accuracy)\n",
      "overestimation = float(len(accuracy[accuracy < 0]))/len(accuracy)\n",
      "print('Percent correct = ' +  str(correctness))\n",
      "print('Percent underestimated = ' +  str(underestimation))\n",
      "print('Percent overestimated = ' +  str(overestimation))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Percent correct = 0.711711711712\n",
        "Percent underestimated = 0.117117117117\n",
        "Percent overestimated = 0.171171171171\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict_comp = known_ratings[n_pairs/2:]\n",
      "predict_totals = [0,0,0]\n",
      "actual_totals = [0,0,0]\n",
      "predict_correct = [0,0,0]\n",
      "for i in range(len(predicted)):\n",
      "    predict_totals[predicted[i] - 1] += 1\n",
      "    actual_totals[predict_comp[i] - 1] += 1\n",
      "    if predicted[i] == predict_comp[i]:\n",
      "        predict_correct[predicted[i] - 1] += 1\n",
      "print(predict_correct)\n",
      "print(predict_totals)\n",
      "print(actual_totals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[144, 0, 251]\n",
        "[239, 0, 316]\n",
        "[191, 59, 305]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_squared_error = float(sum(np.square(accuracy)))/len(accuracy)\n",
      "print('MSE = ' + str(mean_squared_error))\n",
      "mean_absolute_value_error = float(sum(abs(accuracy)))/len(accuracy)\n",
      "print('Mean Error = ' + str(mean_absolute_value_error))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MSE = 0.834234234234\n",
        "Mean Error = 0.47027027027\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}