{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to calculate the IDF vector for words in the dataset\n",
    "\n",
    "### Input: \n",
    "    crunch base and pitch book descriptions of companies\n",
    "### Output:\n",
    "    A pandas dataframe series (column vector), where the index is each word and each row value is the corresponding IDF value (able to write this to csv as well). Sorted in descending order\n",
    "\n",
    "#### Notes: \n",
    " - Currently having issues with time complexity in this solution in regards to calculating the IDF value for all of the 650,000 companies in the entire dataset. I have isolated the inefficiency to numDocsContaining function, which if you look at how it calculates the freq of each word individually it needs a better method, which I will work on \n",
    " \n",
    " - Since the format will be the same, I calculated this IDF vector only for the training set of companies so we can get the code working for building the graph first\n",
    " \n",
    " - This script takes a long time to run, as is, for only ~3000 companies (but with using both the cb and pb descriptions)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 'ConferenceCloud provides state-of-the-art live communications and interactivity for hybrid conferences, lectures and meetings.',\n",
       "        'Provider of an online conferencing platform. The company provides an online platform which allows conducting of video conferences.'],\n",
       "       [ 'Terminus is a platform that seamlessly integrate salesforce CRM and build segments of best fit accounts.',\n",
       "        \"Developer of a B2B advertising platform. The company enables B2B marketers to simplify account-based marketing to reach and engage targeted accounts across all stages of the buyer's journey.\"],\n",
       "       ['Next generation card processing platform',\n",
       "        'Provider of payment processing services. The company provides pre-paid visa, mastercard and bill payment processing services and also offers its clients business intelligence, analytics and program management services.'],\n",
       "       ..., \n",
       "       [ 'Kupu, based in Hawaii, aims to empower the youth to serve the community through character building and environmental stewardship.',\n",
       "        'Operator of a non-profit organization for natural resource management. The company provides training to the next generation in natural resource management, renewable energy, energy conservation and other green job skill sets.'],\n",
       "       [ 'Crowdsourced Market Intelligence Platform to Obtain Customized Market Research',\n",
       "        \"Provider of an on demand research platform. The company's research platform allows professionals to order business research from qualified and vetted top-tier students.\"],\n",
       "       [ 'Satellite Solutions Worldwide continues to revolutionise the satellite broadband communications industry .',\n",
       "        'Provider of satellite-transmission services. The company provides satellite broadband, emergency communication, private circuit connections, back-up broadband connectivity and remote site hook-up services in the United Kingdom and Europe. It operates 3 main satellite platforms, on 10 different satellites.']], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Have a look at the category labeled training data \n",
    "###################### Training Data Test #############################\n",
    "training_categories_df = pd.read_csv(\"../data/category_training_labeled_fixed.csv\",  encoding = \"ISO-8859-1\")\n",
    "mydoclist = training_categories_df.ix[0:,('cb_desc','pb_desc')].values\n",
    "#####################\n",
    "\n",
    "#################### Full Data - Not working currently, need to make IDF calculation more efficient #########################\n",
    "#training_categories_df = pd.read_csv(\"../data/raw_data_fixed.csv\",  encoding = \"ISO-8859-1\", usecols=['domain'\\\n",
    "#, 'tx_industry', 'cb_category', 'tx_category', 'cb_desc', 'pb_desc', 'pb_category'])\n",
    "#mydoclist = training_categories_df.ix[0:,'cb_desc'].values\n",
    "\n",
    "mydoclist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def build_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    iteration = 1\n",
    "    for doc in corpus:\n",
    "        # This line is where you need to clean up the words\n",
    "        lexicon.update([word.strip().lower().translate(str.maketrans('', '', string.punctuation)) for word in str(doc).split()])  \n",
    "        if iteration % 100000 is 0:\n",
    "            print('Iteration {} out of {}'.format(iteration, len(corpus)))\n",
    "        iteration += 1\n",
    "    # Now filter out all of the stop words\n",
    "    print(\"Out of the first loop, into the filter stages\")\n",
    "    filtered = {word for word in lexicon if not word in stopwords.words('english')}\n",
    "    print(\"Finished filtering stop words, now into filtering Nan's\")\n",
    "    # Last, filter out all the nan values and any spaces that made it through\n",
    "    filtered_final = {word for word in filtered if not word in {'nan',' '}}\n",
    "    return filtered_final\n",
    "\n",
    "\n",
    "vocabulary = build_lexicon(mydoclist)\n",
    "\n",
    "\n",
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        #if freq(word, doc) > 0:  # This line seems inefficient because you don't need the full count for this you just need to know it is present at least once\n",
    "        if word in str(doc).split():\n",
    "            doccount +=1\n",
    "    return doccount \n",
    "\n",
    "def freq(term, document):\n",
    "    return str(document).split().count(term)\n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / 1+df)\n",
    "\n",
    "print(\"Finished building the lexicon, now into the idf calculations...\")\n",
    "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
    "print(\"Done!!!\")\n",
    "#print('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "#print('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e17240a9f856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0midf_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_idf_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msorted_idf_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midf_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "idf_vector = pd.Series(my_idf_vector, index=vocabulary)\n",
    "\n",
    "sorted_idf_vector = idf_vector.sort_values(ascending=False)\n",
    "\n",
    "print(sorted_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_idf_vector.to_csv(\"../data/initial_idf_vector.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------- This is the end of the code to calculate the idf vector and write to CSV\n",
    "The below code is a way to generate a matrix where the rows are each document and the columns represent each word, with an entry being the idf value of each word if it is present in the document (not sure if this will be useful or not) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "\n",
    "#This function is titled term frequency, but actually returns a 1 if the word is present in the document and a 0 else\n",
    "def tf(term, document):\n",
    "    count = freq(term, document)\n",
    "    if count >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# We now build the doc_term_matrix, which is a list of lists, where each list is a row with a 1 in each column if the\n",
    "# word that column represents is present in the document\n",
    "doc_term_matrix = []\n",
    "\n",
    "for doc in mydoclist:\n",
    "    print('The doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    #print('The tf vector for Document %d is [%s]' % ((mydoclist.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "\n",
    "doc_term_matrix = np.matrix(doc_term_matrix)\n",
    "print(doc_term_matrix)\n",
    "\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector) # This is a diagonal matrix, where the diagonal is the idf vector found previously\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now create an idf matrix, where the column is each term, and the row represents a document. The value represents\n",
    "# the idf of the term in the document\n",
    "\n",
    "idf_matrix = np.dot(doc_term_matrix, my_idf_matrix)\n",
    "idf_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
